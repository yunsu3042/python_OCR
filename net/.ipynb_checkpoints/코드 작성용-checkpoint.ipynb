{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # coding: utf-8\n",
    "# import sys, os\n",
    "# sys.path.append(os.pardir)  #| 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from dataset.ocrdata import load_dataset\n",
    "# from dataset.mnist import load_mnist\n",
    "# from net.deep_convnet2 import DeepConvNet\n",
    "# from common.trainer import Trainer\n",
    "# import pickle\n",
    "# (x_train, t_train), (x_test, t_test) = load_dataset()\n",
    "\n",
    "# save_path = '/Users/yunsu/Desktop/book_reading/study_by_self/load/ocrparams/E_upper.pk1'\n",
    "# file_path = save_path\n",
    "\n",
    "# network = DeepConvNet(output_size=26, file_path=file_path)  \n",
    "# trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "#                   epochs=5, mini_batch_size=100,\n",
    "#                   optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "#                   evaluate_sample_num_per_epoch=1000)\n",
    "# trainer.train(save_path=save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # coding: utf-8\n",
    "# import sys, os\n",
    "# sys.path.append(os.pardir)  #| 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from dataset.ocrdata2 import load_dataset\n",
    "# from dataset.mnist import load_mnist\n",
    "# from net.deep_convnet3 import DeepConvNet\n",
    "# from common.trainer import Trainer\n",
    "# import pickle\n",
    "# (x_train, t_train), (x_test, t_test) = load_dataset()\n",
    "# save_path = '/Users/yunsu/Desktop/book_reading/study_by_self/load/ocrparams/revised_data/E_D_all_FNx2.pk1'\n",
    "# file_path = None\n",
    "# network = DeepConvNet(output_size=36, file_path=file_path)  \n",
    "# trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "#                   epochs=10, mini_batch_size=100,\n",
    "#                   optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "#                   evaluate_sample_num_per_epoch=1000)\n",
    "# trainer.train(save_path=save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making data pickles\n",
      "done\n",
      "[   9  288  288  576  576 1152 2048  100]\n",
      "0\n",
      "train loss:3.01648577206\n",
      "1th epoch end: Saved Network Parameters!\n",
      "=== epoch:1, train acc:0.099, test acc:0.104 ===\n",
      "1\n",
      "train loss:3.29016686729\n",
      "2\n",
      "train loss:3.36342163624\n",
      "3\n",
      "train loss:3.33133574707\n",
      "4\n",
      "train loss:3.11710349895\n",
      "5\n",
      "train loss:3.44230838349\n",
      "6\n",
      "train loss:3.30479564956\n",
      "7\n",
      "train loss:3.15622962708\n",
      "8\n",
      "train loss:3.16475593264\n",
      "9\n",
      "train loss:3.1031384315\n",
      "10\n",
      "train loss:2.93215036593\n",
      "11\n",
      "train loss:2.74075842891\n",
      "12\n",
      "train loss:2.95919788398\n",
      "13\n",
      "train loss:2.44881849424\n",
      "14\n",
      "train loss:2.44076642198\n",
      "15\n",
      "train loss:2.47521385838\n",
      "16\n",
      "train loss:2.41966224566\n",
      "17\n",
      "train loss:2.28688338214\n",
      "18\n",
      "train loss:2.42840634935\n",
      "19\n",
      "train loss:2.37993144873\n",
      "20\n",
      "train loss:2.22513819854\n",
      "21\n",
      "train loss:2.1072690605\n",
      "22\n",
      "train loss:2.04692151043\n",
      "23\n",
      "train loss:2.02561455961\n",
      "24\n",
      "train loss:1.92935352434\n",
      "25\n",
      "train loss:1.9390557199\n",
      "26\n",
      "train loss:1.9373017023\n",
      "27\n",
      "train loss:1.91098795611\n",
      "28\n",
      "train loss:1.96960087551\n",
      "29\n",
      "train loss:1.81748692981\n",
      "30\n",
      "train loss:1.79433960795\n",
      "31\n",
      "train loss:1.75972637365\n",
      "32\n",
      "train loss:1.67583299819\n",
      "33\n",
      "train loss:1.8108200871\n",
      "34\n",
      "train loss:1.85777587819\n",
      "35\n",
      "train loss:1.80933809138\n",
      "36\n",
      "train loss:1.920479753\n",
      "37\n",
      "train loss:1.81335518438\n",
      "38\n",
      "train loss:1.92946962768\n",
      "39\n",
      "train loss:1.53869726993\n",
      "40\n",
      "train loss:1.49838016335\n",
      "41\n",
      "train loss:1.73504395455\n",
      "42\n",
      "train loss:1.64706657999\n",
      "43\n",
      "train loss:1.7687911681\n",
      "44\n",
      "train loss:1.54842501405\n",
      "45\n",
      "train loss:1.64121764321\n",
      "46\n",
      "train loss:1.54222285427\n",
      "47\n",
      "train loss:1.74816124566\n",
      "48\n",
      "train loss:1.47792591167\n",
      "49\n",
      "train loss:1.3419964028\n",
      "50\n",
      "train loss:1.35448521069\n",
      "51\n",
      "train loss:1.43519602111\n",
      "52\n",
      "train loss:1.27139634623\n",
      "53\n",
      "train loss:1.42117436915\n",
      "54\n",
      "train loss:1.42655823088\n",
      "55\n",
      "train loss:1.47227509323\n",
      "56\n",
      "train loss:1.35429728319\n",
      "57\n",
      "train loss:1.42485628955\n",
      "58\n",
      "train loss:1.38952162309\n",
      "59\n",
      "train loss:1.40198257328\n",
      "60\n",
      "train loss:1.35903477161\n",
      "61\n",
      "train loss:1.28432353214\n",
      "62\n",
      "train loss:1.36356060483\n",
      "63\n",
      "train loss:1.26626408326\n",
      "64\n",
      "train loss:1.35592257192\n",
      "65\n",
      "train loss:1.25765801345\n",
      "66\n",
      "train loss:1.22311564172\n",
      "67\n",
      "train loss:1.29558678982\n",
      "68\n",
      "train loss:1.19271244436\n",
      "69\n",
      "train loss:1.43801454771\n",
      "70\n",
      "train loss:1.22964630023\n",
      "71\n",
      "train loss:1.17306266491\n",
      "72\n",
      "train loss:1.32940386557\n",
      "73\n",
      "train loss:1.28355834367\n",
      "74\n",
      "train loss:1.46724993524\n",
      "75\n",
      "train loss:1.13733683923\n",
      "76\n",
      "train loss:1.33884490215\n",
      "77\n",
      "train loss:1.15962314051\n",
      "78\n",
      "train loss:1.10677762555\n",
      "79\n",
      "train loss:1.28419851439\n",
      "80\n",
      "train loss:1.22334210338\n",
      "81\n",
      "train loss:1.21029266915\n",
      "82\n",
      "train loss:1.00691372164\n",
      "83\n",
      "train loss:1.16178005371\n",
      "84\n",
      "train loss:1.21402949493\n",
      "85\n",
      "train loss:1.03295089754\n",
      "86\n",
      "train loss:1.09983671343\n",
      "87\n",
      "train loss:1.17501037015\n",
      "88\n",
      "train loss:1.16355446971\n",
      "89\n",
      "train loss:1.02939800524\n",
      "90\n",
      "train loss:1.18932322307\n",
      "91\n",
      "train loss:1.1884555403\n",
      "92\n",
      "train loss:1.14326132685\n",
      "93\n",
      "train loss:1.07209263701\n",
      "94\n",
      "train loss:1.130186497\n",
      "95\n",
      "train loss:1.04165410564\n",
      "96\n",
      "train loss:1.15689898109\n",
      "97\n",
      "train loss:1.13978637276\n",
      "98\n",
      "train loss:1.0720331433\n",
      "99\n",
      "train loss:1.21542552825\n",
      "100\n",
      "train loss:1.02463601173\n",
      "101\n",
      "train loss:1.25112142626\n",
      "102\n",
      "train loss:1.33527468896\n",
      "103\n",
      "train loss:1.03099165857\n",
      "104\n",
      "train loss:1.07364395562\n",
      "105\n",
      "train loss:0.95566484106\n",
      "106\n",
      "train loss:1.12507000022\n",
      "107\n",
      "train loss:0.883073919179\n",
      "108\n",
      "train loss:1.19409775961\n",
      "109\n",
      "train loss:1.05273686007\n",
      "110\n",
      "train loss:1.04476605038\n",
      "111\n",
      "train loss:0.978729233157\n",
      "112\n",
      "train loss:1.03224481622\n",
      "113\n",
      "train loss:0.984475668482\n",
      "114\n",
      "train loss:1.04743970609\n",
      "115\n",
      "train loss:1.02809129481\n",
      "116\n",
      "train loss:1.02775622958\n",
      "117\n",
      "train loss:1.173380506\n",
      "118\n",
      "train loss:0.978046371475\n",
      "119\n",
      "train loss:1.11769973238\n",
      "120\n",
      "train loss:1.19767449014\n",
      "121\n",
      "train loss:1.18079726942\n",
      "122\n",
      "train loss:0.980863516421\n",
      "123\n",
      "train loss:1.30139893733\n",
      "124\n",
      "train loss:1.14086712332\n",
      "125\n",
      "train loss:0.955828602688\n",
      "126\n",
      "train loss:0.981561338008\n",
      "127\n",
      "train loss:0.93502858475\n",
      "128\n",
      "train loss:0.928998432292\n",
      "129\n",
      "train loss:1.01660535489\n",
      "130\n",
      "train loss:0.995447810313\n",
      "131\n",
      "train loss:1.01597267767\n",
      "132\n",
      "train loss:1.14891646876\n",
      "133\n",
      "train loss:0.909190637089\n",
      "134\n",
      "train loss:1.01125134963\n",
      "135\n",
      "train loss:1.05595802728\n",
      "136\n",
      "train loss:1.06951944623\n",
      "137\n",
      "train loss:1.1598949719\n",
      "138\n",
      "train loss:0.863112187146\n",
      "139\n",
      "train loss:0.974023156823\n",
      "140\n",
      "train loss:0.950917855793\n",
      "141\n",
      "train loss:0.906117573082\n",
      "142\n",
      "train loss:1.09660179774\n",
      "143\n",
      "train loss:0.950350014185\n",
      "144\n",
      "train loss:0.965662866961\n",
      "145\n",
      "train loss:1.10603210167\n",
      "146\n",
      "train loss:0.931120109494\n",
      "147\n",
      "train loss:0.908020883528\n",
      "148\n",
      "train loss:0.948425913467\n",
      "149\n",
      "train loss:0.959698973259\n",
      "150\n",
      "train loss:0.958318299765\n",
      "151\n",
      "train loss:0.991496868696\n",
      "152\n",
      "train loss:1.10153958612\n",
      "153\n",
      "train loss:0.972681609775\n",
      "154\n",
      "train loss:0.909059803465\n",
      "155\n",
      "train loss:0.804958197575\n",
      "156\n",
      "train loss:0.931125012905\n",
      "157\n",
      "train loss:0.926177392694\n",
      "158\n",
      "train loss:1.03315749955\n",
      "159\n",
      "train loss:0.970549986607\n",
      "160\n",
      "train loss:1.04073289081\n",
      "161\n",
      "train loss:0.867866987953\n",
      "162\n",
      "train loss:0.819961923716\n",
      "163\n",
      "train loss:0.780214425978\n",
      "164\n",
      "train loss:0.904636755803\n",
      "165\n",
      "train loss:0.876327130564\n",
      "166\n",
      "train loss:1.2101647943\n",
      "167\n",
      "train loss:0.879370057527\n",
      "168\n",
      "train loss:1.0024487629\n",
      "169\n",
      "train loss:0.791551461861\n",
      "170\n",
      "train loss:0.852960476388\n",
      "171\n",
      "train loss:0.926028889926\n",
      "172\n",
      "train loss:0.835671958369\n",
      "173\n",
      "train loss:0.827464768301\n",
      "174\n",
      "train loss:0.810382549852\n",
      "175\n",
      "train loss:0.926718183962\n",
      "176\n",
      "train loss:1.02529335646\n",
      "177\n",
      "train loss:0.690549008211\n",
      "178\n",
      "train loss:0.943633633356\n",
      "179\n",
      "train loss:0.769685687736\n",
      "180\n",
      "train loss:0.867287258302\n",
      "181\n",
      "train loss:0.90173864777\n",
      "182\n",
      "train loss:0.842298575639\n",
      "183\n",
      "train loss:1.00486293734\n",
      "184\n",
      "train loss:0.835178454205\n",
      "185\n",
      "train loss:0.901416279251\n",
      "186\n",
      "train loss:0.861255289465\n",
      "187\n",
      "train loss:0.929473717375\n",
      "188\n",
      "train loss:0.854623199175\n",
      "189\n",
      "train loss:0.815356545537\n",
      "190\n",
      "train loss:1.01176367381\n",
      "191\n",
      "train loss:0.786703778414\n",
      "192\n",
      "train loss:1.05850908992\n",
      "193\n",
      "train loss:0.860049308064\n",
      "194\n",
      "train loss:0.884946318732\n",
      "195\n",
      "train loss:1.05913927447\n",
      "196\n",
      "train loss:0.702314825544\n",
      "197\n",
      "train loss:0.88316191985\n",
      "198\n",
      "train loss:0.841020823148\n",
      "199\n",
      "train loss:0.768997764567\n",
      "200\n",
      "train loss:0.904598446662\n",
      "201\n",
      "train loss:0.82191759696\n",
      "202\n",
      "train loss:0.82554384773\n",
      "203\n",
      "train loss:0.850221714933\n",
      "204\n",
      "train loss:0.879633732403\n",
      "205\n",
      "train loss:0.826853716867\n",
      "206\n",
      "train loss:0.814786006671\n",
      "207\n",
      "train loss:0.930685921127\n",
      "208\n",
      "train loss:0.803144608264\n",
      "209\n",
      "train loss:0.969601337551\n",
      "210\n",
      "train loss:0.740677965597\n",
      "211\n",
      "train loss:0.748498379171\n",
      "212\n",
      "train loss:0.755833557946\n",
      "213\n",
      "train loss:0.854882905185\n",
      "214\n",
      "train loss:0.740158241384\n",
      "215\n",
      "train loss:0.780029743986\n",
      "216\n",
      "train loss:0.894195916216\n",
      "217\n",
      "train loss:0.930710600203\n",
      "218\n",
      "train loss:0.965746462816\n",
      "219\n",
      "train loss:0.801982029014\n",
      "220\n",
      "train loss:0.866839900877\n",
      "221\n",
      "train loss:0.754717039112\n",
      "222\n",
      "train loss:0.781024019887\n",
      "223\n",
      "train loss:0.913156579877\n",
      "224\n",
      "train loss:0.856700326532\n",
      "225\n",
      "train loss:0.741964101225\n",
      "226\n",
      "train loss:0.707456647292\n",
      "227\n",
      "train loss:1.10182361919\n",
      "228\n",
      "train loss:0.890621230848\n",
      "229\n",
      "train loss:0.770416744277\n",
      "230\n",
      "train loss:0.837321518546\n",
      "231\n",
      "train loss:0.726429229982\n",
      "232\n",
      "train loss:0.871408465573\n",
      "233\n",
      "train loss:0.817555859564\n",
      "234\n",
      "train loss:0.77268261471\n",
      "235\n",
      "train loss:0.821946343086\n",
      "236\n",
      "train loss:0.72876154692\n",
      "237\n",
      "train loss:0.794413380957\n",
      "238\n",
      "train loss:0.783816167222\n",
      "239\n",
      "train loss:0.832140719217\n",
      "240\n",
      "train loss:0.847110552092\n",
      "241\n",
      "train loss:0.878732050039\n",
      "242\n",
      "train loss:0.743648847564\n",
      "243\n",
      "train loss:0.70857653061\n",
      "244\n",
      "train loss:0.767751047481\n",
      "245\n",
      "train loss:0.862782953712\n",
      "246\n",
      "train loss:0.778533898055\n",
      "247\n",
      "train loss:0.797228985583\n",
      "248\n",
      "train loss:0.945896254949\n",
      "249\n",
      "train loss:0.918872282874\n",
      "250\n",
      "train loss:0.699718852559\n",
      "251\n",
      "train loss:0.814675903999\n",
      "252\n",
      "train loss:0.927059953221\n",
      "253\n",
      "train loss:0.924640243356\n",
      "254\n",
      "train loss:0.917305033727\n",
      "255\n",
      "train loss:0.881902351587\n",
      "256\n",
      "train loss:0.816684938683\n",
      "257\n",
      "train loss:0.803556264251\n",
      "258\n",
      "train loss:0.883552602629\n",
      "259\n",
      "train loss:0.599807663289\n",
      "260\n",
      "train loss:0.654719366867\n",
      "261\n",
      "train loss:0.837839169997\n",
      "262\n",
      "train loss:0.844181027617\n",
      "263\n",
      "train loss:0.775980212935\n",
      "264\n",
      "train loss:0.795437546677\n",
      "265\n",
      "train loss:0.679120490798\n",
      "266\n",
      "train loss:0.59996127203\n",
      "0\n",
      "train loss:0.774857336875\n",
      "2th epoch end: Saved Network Parameters!\n",
      "=== epoch:2, train acc:0.976, test acc:0.963 ===\n",
      "1\n",
      "train loss:0.65677554004\n",
      "2\n",
      "train loss:0.805302377208\n",
      "3\n",
      "train loss:0.700997344654\n",
      "4\n",
      "train loss:0.658028155389\n",
      "5\n",
      "train loss:0.795023900379\n",
      "6\n",
      "train loss:0.792120005212\n",
      "7\n",
      "train loss:0.782052669426\n",
      "8\n",
      "train loss:0.845966976595\n",
      "9\n",
      "train loss:0.682032175989\n",
      "10\n",
      "train loss:0.595218316399\n",
      "11\n",
      "train loss:0.728580756301\n",
      "12\n",
      "train loss:0.693059181752\n",
      "13\n",
      "train loss:0.64468221966\n",
      "14\n",
      "train loss:0.67578868299\n",
      "15\n",
      "train loss:0.77229946105\n",
      "16\n",
      "train loss:0.676456408103\n",
      "17\n",
      "train loss:0.807412655745\n",
      "18\n",
      "train loss:0.585079374146\n",
      "19\n",
      "train loss:0.667986198423\n",
      "20\n",
      "train loss:0.869865202561\n",
      "21\n",
      "train loss:0.753038305016\n",
      "22\n",
      "train loss:0.77749582262\n",
      "23\n",
      "train loss:0.856032292674\n",
      "24\n",
      "train loss:0.915127136931\n",
      "25\n",
      "train loss:0.782648428428\n",
      "26\n",
      "train loss:0.72737704653\n",
      "27\n",
      "train loss:0.813491045488\n",
      "28\n",
      "train loss:0.754093335356\n",
      "29\n",
      "train loss:0.906745256469\n",
      "30\n",
      "train loss:0.864349195683\n",
      "31\n",
      "train loss:0.853760425377\n",
      "32\n",
      "train loss:0.753634964608\n",
      "33\n",
      "train loss:0.663114156027\n",
      "34\n",
      "train loss:0.803981242093\n",
      "35\n",
      "train loss:0.765845338525\n",
      "36\n",
      "train loss:0.736582300044\n",
      "37\n",
      "train loss:0.753027813251\n",
      "38\n",
      "train loss:0.709712971821\n",
      "39\n",
      "train loss:0.824050781035\n",
      "40\n",
      "train loss:0.821606200238\n",
      "41\n",
      "train loss:0.648654494344\n",
      "42\n",
      "train loss:0.838778831281\n",
      "43\n",
      "train loss:0.70301675173\n",
      "44\n",
      "train loss:0.794508776326\n",
      "45\n",
      "train loss:0.694800329286\n",
      "46\n",
      "train loss:0.7396726049\n",
      "47\n",
      "train loss:0.798938210942\n",
      "48\n",
      "train loss:0.765358773928\n",
      "49\n",
      "train loss:0.736531499578\n",
      "50\n",
      "train loss:0.710157095958\n",
      "51\n",
      "train loss:0.65674465799\n",
      "52\n",
      "train loss:0.796397481595\n",
      "53\n",
      "train loss:0.883614684611\n",
      "54\n",
      "train loss:0.771355127115\n",
      "55\n",
      "train loss:0.791997245106\n",
      "56\n",
      "train loss:0.603712205424\n",
      "57\n",
      "train loss:0.699984607765\n",
      "58\n",
      "train loss:0.750406127193\n",
      "59\n",
      "train loss:0.722971665132\n",
      "60\n",
      "train loss:0.710021166758\n",
      "61\n",
      "train loss:0.700962771735\n",
      "62\n",
      "train loss:0.676336881104\n",
      "63\n",
      "train loss:0.810513532246\n",
      "64\n",
      "train loss:0.748128883827\n",
      "65\n",
      "train loss:0.735952907364\n",
      "66\n",
      "train loss:0.713693065168\n",
      "67\n",
      "train loss:0.798306873072\n",
      "68\n",
      "train loss:0.781037821272\n",
      "69\n",
      "train loss:0.764680096598\n",
      "70\n",
      "train loss:0.784679666056\n",
      "71\n",
      "train loss:0.802381070246\n",
      "72\n",
      "train loss:0.670557475884\n",
      "73\n",
      "train loss:0.614093541083\n",
      "74\n",
      "train loss:0.830031604291\n",
      "75\n",
      "train loss:0.556986349107\n",
      "76\n",
      "train loss:0.748994172369\n",
      "77\n",
      "train loss:0.604343509994\n",
      "78\n",
      "train loss:0.699016321383\n",
      "79\n",
      "train loss:0.823689731963\n",
      "80\n",
      "train loss:0.673364665611\n",
      "81\n",
      "train loss:0.735293912023\n",
      "82\n",
      "train loss:0.69242058928\n",
      "83\n",
      "train loss:0.689616299522\n",
      "84\n",
      "train loss:0.826219939124\n",
      "85\n",
      "train loss:0.734379775891\n",
      "86\n",
      "train loss:0.797509518827\n",
      "87\n",
      "train loss:0.682213901783\n",
      "88\n",
      "train loss:0.730764183742\n",
      "89\n",
      "train loss:0.727791387202\n",
      "90\n",
      "train loss:0.669310260658\n",
      "91\n",
      "train loss:0.697434564086\n",
      "92\n",
      "train loss:0.869164646848\n",
      "93\n",
      "train loss:0.853328801782\n",
      "94\n",
      "train loss:0.705424881982\n",
      "95\n",
      "train loss:0.797999726732\n",
      "96\n",
      "train loss:0.708261150997\n",
      "97\n",
      "train loss:0.76720668675\n",
      "98\n",
      "train loss:0.814105741288\n",
      "99\n",
      "train loss:0.885307167295\n",
      "100\n",
      "train loss:0.753330295675\n",
      "101\n",
      "train loss:0.631779624709\n",
      "102\n",
      "train loss:0.6177984095\n",
      "103\n",
      "train loss:0.66364855942\n",
      "104\n",
      "train loss:0.691659943048\n",
      "105\n",
      "train loss:0.820967362572\n",
      "106\n",
      "train loss:0.866797899746\n",
      "107\n",
      "train loss:0.683564318498\n",
      "108\n",
      "train loss:0.572493323827\n",
      "109\n",
      "train loss:0.810564200753\n",
      "110\n",
      "train loss:0.826536156584\n",
      "111\n",
      "train loss:0.682466971197\n",
      "112\n",
      "train loss:0.7443020339\n",
      "113\n",
      "train loss:0.676280766272\n",
      "114\n",
      "train loss:0.663433508355\n",
      "115\n",
      "train loss:0.8190265797\n",
      "116\n",
      "train loss:0.691418433066\n",
      "117\n",
      "train loss:0.720864592203\n",
      "118\n",
      "train loss:0.768517400038\n",
      "119\n",
      "train loss:0.628726670626\n",
      "120\n",
      "train loss:0.753600069656\n",
      "121\n",
      "train loss:0.587691428647\n",
      "122\n",
      "train loss:0.699725761809\n",
      "123\n",
      "train loss:0.715333296901\n",
      "124\n",
      "train loss:0.688990480166\n",
      "125\n",
      "train loss:0.754077046359\n",
      "126\n",
      "train loss:0.681307549873\n",
      "127\n",
      "train loss:0.688793045077\n",
      "128\n",
      "train loss:0.631808322531\n",
      "129\n",
      "train loss:0.825339451343\n",
      "130\n",
      "train loss:0.826408684223\n",
      "131\n",
      "train loss:0.716088059802\n",
      "132\n",
      "train loss:0.60356506512\n",
      "133\n",
      "train loss:0.693159664064\n",
      "134\n",
      "train loss:0.626762043183\n",
      "135\n",
      "train loss:0.693771299975\n",
      "136\n",
      "train loss:0.719497334429\n",
      "137\n",
      "train loss:0.595190928375\n",
      "138\n",
      "train loss:0.851765487308\n",
      "139\n",
      "train loss:0.806334393405\n",
      "140\n",
      "train loss:0.775527901894\n",
      "141\n",
      "train loss:0.580370029002\n",
      "142\n",
      "train loss:0.52332919205\n",
      "143\n",
      "train loss:0.659494517079\n",
      "144\n",
      "train loss:0.632879985657\n",
      "145\n",
      "train loss:0.850767412616\n",
      "146\n",
      "train loss:0.681882349663\n",
      "147\n",
      "train loss:0.734333051697\n",
      "148\n",
      "train loss:0.570649099926\n",
      "149\n",
      "train loss:0.700574548401\n",
      "150\n",
      "train loss:0.770645882937\n",
      "151\n",
      "train loss:0.589220744823\n",
      "152\n",
      "train loss:0.637841045017\n",
      "153\n",
      "train loss:0.741228433576\n",
      "154\n",
      "train loss:0.866835609381\n",
      "155\n",
      "train loss:0.616001548368\n",
      "156\n",
      "train loss:0.625410365449\n",
      "157\n",
      "train loss:0.624585084484\n",
      "158\n",
      "train loss:0.588391708949\n",
      "159\n",
      "train loss:0.674816226635\n",
      "160\n",
      "train loss:0.695716592488\n",
      "161\n",
      "train loss:0.727208008879\n",
      "162\n",
      "train loss:0.765997341968\n",
      "163\n",
      "train loss:0.586821852756\n",
      "164\n",
      "train loss:0.607616479818\n",
      "165\n",
      "train loss:0.723923497898\n",
      "166\n",
      "train loss:0.669025254826\n",
      "167\n",
      "train loss:0.671416709911\n",
      "168\n",
      "train loss:0.593580417065\n",
      "169\n",
      "train loss:0.816043280147\n",
      "170\n",
      "train loss:0.644364595813\n",
      "171\n",
      "train loss:0.604077754724\n",
      "172\n",
      "train loss:0.764864542403\n",
      "173\n",
      "train loss:0.627417788194\n",
      "174\n",
      "train loss:0.610448844819\n",
      "175\n",
      "train loss:0.625251666331\n",
      "176\n",
      "train loss:0.557893453206\n",
      "177\n",
      "train loss:0.681491732941\n",
      "178\n",
      "train loss:0.67620626084\n",
      "179\n",
      "train loss:0.571025909335\n",
      "180\n",
      "train loss:0.620800412802\n",
      "181\n",
      "train loss:0.51058201489\n",
      "182\n",
      "train loss:0.585579197393\n",
      "183\n",
      "train loss:0.839215381799\n",
      "184\n",
      "train loss:0.67944235885\n",
      "185\n",
      "train loss:0.661885632374\n",
      "186\n",
      "train loss:0.718472672336\n",
      "187\n",
      "train loss:0.650315550468\n",
      "188\n",
      "train loss:0.610331172081\n",
      "189\n",
      "train loss:0.663493109518\n",
      "190\n",
      "train loss:0.783469220466\n",
      "191\n",
      "train loss:0.775054716906\n",
      "192\n",
      "train loss:0.619736012023\n",
      "193\n",
      "train loss:0.693640522956\n",
      "194\n",
      "train loss:0.762103659154\n",
      "195\n",
      "train loss:0.584174375492\n",
      "196\n",
      "train loss:0.705492810724\n",
      "197\n",
      "train loss:0.8174545251\n",
      "198\n",
      "train loss:0.615349918086\n",
      "199\n",
      "train loss:0.639746972\n",
      "200\n",
      "train loss:0.625231010443\n",
      "201\n",
      "train loss:0.607232156209\n",
      "202\n",
      "train loss:0.672114280513\n",
      "203\n",
      "train loss:0.640332381556\n",
      "204\n",
      "train loss:0.55048233894\n",
      "205\n",
      "train loss:0.621586737311\n",
      "206\n",
      "train loss:0.699014062215\n",
      "207\n",
      "train loss:0.674086331213\n",
      "208\n",
      "train loss:0.652542756883\n",
      "209\n",
      "train loss:0.688207231152\n",
      "210\n",
      "train loss:0.744114302197\n",
      "211\n",
      "train loss:0.662585774557\n",
      "212\n",
      "train loss:0.672359247216\n",
      "213\n",
      "train loss:0.678612139305\n",
      "214\n",
      "train loss:0.628434195832\n",
      "215\n",
      "train loss:0.75206436551\n",
      "216\n",
      "train loss:0.553733214994\n",
      "217\n",
      "train loss:0.653478472533\n",
      "218\n",
      "train loss:0.61953444865\n",
      "219\n",
      "train loss:0.642821808199\n",
      "220\n",
      "train loss:0.740752623606\n",
      "221\n",
      "train loss:0.749177385363\n",
      "222\n",
      "train loss:0.738984101624\n",
      "223\n",
      "train loss:0.634283188681\n",
      "224\n",
      "train loss:0.706908234938\n",
      "225\n",
      "train loss:0.580270113873\n",
      "226\n",
      "train loss:0.56890567221\n",
      "227\n",
      "train loss:0.692430658092\n",
      "228\n",
      "train loss:0.823079200103\n",
      "229\n",
      "train loss:0.569246996184\n",
      "230\n",
      "train loss:0.660823638903\n",
      "231\n",
      "train loss:0.562471294691\n",
      "232\n",
      "train loss:0.578849163538\n",
      "233\n",
      "train loss:0.645455450932\n",
      "234\n",
      "train loss:0.729571497542\n",
      "235\n",
      "train loss:0.69073628446\n",
      "236\n",
      "train loss:0.519493207906\n",
      "237\n",
      "train loss:0.642946032459\n",
      "238\n",
      "train loss:0.552860512573\n",
      "239\n",
      "train loss:0.653573047684\n",
      "240\n",
      "train loss:0.712140067462\n",
      "241\n",
      "train loss:0.576046608739\n",
      "242\n",
      "train loss:0.642037167262\n",
      "243\n",
      "train loss:0.543050250809\n",
      "244\n",
      "train loss:0.703021554363\n",
      "245\n",
      "train loss:0.623121032419\n",
      "246\n",
      "train loss:0.625810947786\n",
      "247\n",
      "train loss:0.562781891548\n",
      "248\n",
      "train loss:0.639805472031\n",
      "249\n",
      "train loss:0.736221500243\n",
      "250\n",
      "train loss:0.638359111726\n",
      "251\n",
      "train loss:0.631234113295\n",
      "252\n",
      "train loss:0.633818825155\n",
      "253\n",
      "train loss:0.565555013175\n",
      "254\n",
      "train loss:0.617484432045\n",
      "255\n",
      "train loss:0.652716694444\n",
      "256\n",
      "train loss:0.698116133343\n",
      "257\n",
      "train loss:0.668621924633\n",
      "258\n",
      "train loss:0.618240992293\n",
      "259\n",
      "train loss:0.672257506171\n",
      "260\n",
      "train loss:0.727716726431\n",
      "261\n",
      "train loss:0.631802084748\n",
      "262\n",
      "train loss:0.591150608564\n",
      "263\n",
      "train loss:0.5900512965\n",
      "264\n",
      "train loss:0.704052638258\n",
      "265\n",
      "train loss:0.616377050648\n",
      "266\n",
      "train loss:0.715343093517\n",
      "0\n",
      "train loss:0.696177252008\n",
      "3th epoch end: Saved Network Parameters!\n",
      "=== epoch:3, train acc:0.991, test acc:0.983 ===\n",
      "1\n",
      "train loss:0.545767900072\n",
      "2\n",
      "train loss:0.802787067955\n",
      "3\n",
      "train loss:0.61930894813\n",
      "4\n",
      "train loss:0.642482457627\n",
      "5\n",
      "train loss:0.624509309807\n",
      "6\n",
      "train loss:0.52845794424\n",
      "7\n",
      "train loss:0.610129204968\n",
      "8\n",
      "train loss:0.563211198794\n",
      "9\n",
      "train loss:0.678365966414\n",
      "10\n",
      "train loss:0.789266456068\n",
      "11\n",
      "train loss:0.677509732997\n",
      "12\n",
      "train loss:0.684103275071\n",
      "13\n",
      "train loss:0.565585203507\n",
      "14\n",
      "train loss:0.648022570968\n",
      "15\n",
      "train loss:0.529905173174\n",
      "16\n",
      "train loss:0.561215412393\n",
      "17\n",
      "train loss:0.588866162542\n",
      "18\n",
      "train loss:0.53835360979\n",
      "19\n",
      "train loss:0.590278277582\n",
      "20\n",
      "train loss:0.699217430821\n",
      "21\n",
      "train loss:0.643217793894\n",
      "22\n",
      "train loss:0.689221391156\n",
      "23\n",
      "train loss:0.621924025393\n",
      "24\n",
      "train loss:0.699018870315\n",
      "25\n",
      "train loss:0.612857067552\n",
      "26\n",
      "train loss:0.612296704479\n",
      "27\n",
      "train loss:0.533297427997\n",
      "28\n",
      "train loss:0.590076062627\n",
      "29\n",
      "train loss:0.641426751885\n",
      "30\n",
      "train loss:0.633462778799\n",
      "31\n",
      "train loss:0.5516884038\n",
      "32\n",
      "train loss:0.561523011355\n",
      "33\n",
      "train loss:0.537215674863\n",
      "34\n",
      "train loss:0.648794166122\n",
      "35\n",
      "train loss:0.652453034501\n",
      "36\n",
      "train loss:0.697069966226\n",
      "37\n",
      "train loss:0.655629143458\n",
      "38\n",
      "train loss:0.66586464982\n",
      "39\n",
      "train loss:0.548895884437\n",
      "40\n",
      "train loss:0.547629627991\n",
      "41\n",
      "train loss:0.532521723576\n",
      "42\n",
      "train loss:0.534200404063\n",
      "43\n",
      "train loss:0.685785808278\n",
      "44\n",
      "train loss:0.614111943914\n",
      "45\n",
      "train loss:0.727960440635\n",
      "46\n",
      "train loss:0.608346553785\n",
      "47\n",
      "train loss:0.724754612611\n",
      "48\n",
      "train loss:0.581643252815\n",
      "49\n",
      "train loss:0.570865866646\n",
      "50\n",
      "train loss:0.727343873419\n",
      "51\n",
      "train loss:0.614328788775\n",
      "52\n",
      "train loss:0.772080617884\n",
      "53\n",
      "train loss:0.636281631027\n",
      "54\n",
      "train loss:0.645612873849\n",
      "55\n",
      "train loss:0.554006440502\n",
      "56\n",
      "train loss:0.609196143859\n",
      "57\n",
      "train loss:0.582907938204\n",
      "58\n",
      "train loss:0.642482148203\n",
      "59\n",
      "train loss:0.582938370577\n",
      "60\n",
      "train loss:0.623745267786\n",
      "61\n",
      "train loss:0.545450109258\n",
      "62\n",
      "train loss:0.687246933956\n",
      "63\n",
      "train loss:0.57704495461\n",
      "64\n",
      "train loss:0.564927972166\n",
      "65\n",
      "train loss:0.639328107335\n",
      "66\n",
      "train loss:0.652300155648\n",
      "67\n",
      "train loss:0.649555952196\n",
      "68\n",
      "train loss:0.594102046168\n",
      "69\n",
      "train loss:0.587972416887\n",
      "70\n",
      "train loss:0.643063895567\n",
      "71\n",
      "train loss:0.669235868056\n",
      "72\n",
      "train loss:0.659693960074\n",
      "73\n",
      "train loss:0.577059863545\n",
      "74\n",
      "train loss:0.462989449855\n",
      "75\n",
      "train loss:0.670006911376\n",
      "76\n",
      "train loss:0.592380460384\n",
      "77\n",
      "train loss:0.593422756847\n",
      "78\n",
      "train loss:0.696813807769\n",
      "79\n",
      "train loss:0.562529042828\n",
      "80\n",
      "train loss:0.613507968817\n",
      "81\n",
      "train loss:0.690722707749\n",
      "82\n",
      "train loss:0.57313212321\n",
      "83\n",
      "train loss:0.659853288279\n",
      "84\n",
      "train loss:0.539981851513\n",
      "85\n",
      "train loss:0.607205199114\n",
      "86\n",
      "train loss:0.555111189708\n",
      "87\n",
      "train loss:0.668322792457\n",
      "88\n",
      "train loss:0.663039237312\n",
      "89\n",
      "train loss:0.64462920393\n",
      "90\n",
      "train loss:0.728259730821\n",
      "91\n",
      "train loss:0.625659201833\n",
      "92\n",
      "train loss:0.597809246825\n",
      "93\n",
      "train loss:0.613366534287\n",
      "94\n",
      "train loss:0.728966482169\n",
      "95\n",
      "train loss:0.599611315364\n",
      "96\n",
      "train loss:0.520449099359\n",
      "97\n",
      "train loss:0.632635950595\n",
      "98\n",
      "train loss:0.658489319372\n",
      "99\n",
      "train loss:0.665271964053\n",
      "100\n",
      "train loss:0.634623042306\n",
      "101\n",
      "train loss:0.623347423562\n",
      "102\n",
      "train loss:0.670716572733\n",
      "103\n",
      "train loss:0.654099450467\n",
      "104\n",
      "train loss:0.677977726344\n",
      "105\n",
      "train loss:0.468805653303\n",
      "106\n",
      "train loss:0.627222488116\n",
      "107\n",
      "train loss:0.624100831939\n",
      "108\n",
      "train loss:0.558799900855\n",
      "109\n",
      "train loss:0.639240820143\n",
      "110\n",
      "train loss:0.504545814516\n",
      "111\n",
      "train loss:0.589840828489\n",
      "112\n",
      "train loss:0.70144628693\n",
      "113\n",
      "train loss:0.556183409801\n",
      "114\n",
      "train loss:0.65441520247\n",
      "115\n",
      "train loss:0.518250791348\n",
      "116\n",
      "train loss:0.541794166285\n",
      "117\n",
      "train loss:0.705341833444\n",
      "118\n",
      "train loss:0.596711370158\n",
      "119\n",
      "train loss:0.515350190104\n",
      "120\n",
      "train loss:0.584423957161\n",
      "121\n",
      "train loss:0.604643126902\n",
      "122\n",
      "train loss:0.590625523997\n",
      "123\n",
      "train loss:0.544703697038\n",
      "124\n",
      "train loss:0.576550366896\n",
      "125\n",
      "train loss:0.707077168829\n",
      "126\n",
      "train loss:0.627615629437\n",
      "127\n",
      "train loss:0.691077800159\n",
      "128\n",
      "train loss:0.515283437591\n",
      "129\n",
      "train loss:0.715291847549\n",
      "130\n",
      "train loss:0.667938521897\n",
      "131\n",
      "train loss:0.535249182449\n",
      "132\n",
      "train loss:0.528645700531\n",
      "133\n",
      "train loss:0.565953684557\n",
      "134\n",
      "train loss:0.548229158623\n",
      "135\n",
      "train loss:0.519739701636\n",
      "136\n",
      "train loss:0.544510047979\n",
      "137\n",
      "train loss:0.679527391336\n",
      "138\n",
      "train loss:0.635054310414\n",
      "139\n",
      "train loss:0.573596780837\n",
      "140\n",
      "train loss:0.498010669047\n",
      "141\n",
      "train loss:0.573342222501\n",
      "142\n",
      "train loss:0.579580865022\n",
      "143\n",
      "train loss:0.702953670919\n",
      "144\n",
      "train loss:0.682094353028\n",
      "145\n",
      "train loss:0.580354308064\n",
      "146\n",
      "train loss:0.64866767246\n",
      "147\n",
      "train loss:0.642194719557\n",
      "148\n",
      "train loss:0.637427076138\n",
      "149\n",
      "train loss:0.687400143095\n",
      "150\n",
      "train loss:0.676016049745\n",
      "151\n",
      "train loss:0.694810919551\n",
      "152\n",
      "train loss:0.523604976066\n",
      "153\n",
      "train loss:0.566065988237\n",
      "154\n",
      "train loss:0.663715529554\n",
      "155\n",
      "train loss:0.646062964807\n",
      "156\n",
      "train loss:0.505797365868\n",
      "157\n",
      "train loss:0.576044360634\n",
      "158\n",
      "train loss:0.572958598943\n",
      "159\n",
      "train loss:0.540122845768\n",
      "160\n",
      "train loss:0.633294855752\n",
      "161\n",
      "train loss:0.729573772712\n",
      "162\n",
      "train loss:0.691646540325\n",
      "163\n",
      "train loss:0.674392524387\n",
      "164\n",
      "train loss:0.676013886935\n",
      "165\n",
      "train loss:0.666561649728\n",
      "166\n",
      "train loss:0.676610286642\n",
      "167\n",
      "train loss:0.705981258855\n",
      "168\n",
      "train loss:0.652649548758\n",
      "169\n",
      "train loss:0.625117343874\n",
      "170\n",
      "train loss:0.763976770649\n",
      "171\n",
      "train loss:0.471305463257\n",
      "172\n",
      "train loss:0.656537561075\n",
      "173\n",
      "train loss:0.575903493304\n",
      "174\n",
      "train loss:0.655769326229\n",
      "175\n",
      "train loss:0.710468750239\n",
      "176\n",
      "train loss:0.603767940468\n",
      "177\n",
      "train loss:0.424623646663\n",
      "178\n",
      "train loss:0.713996544471\n",
      "179\n",
      "train loss:0.541863634075\n",
      "180\n",
      "train loss:0.608731228235\n",
      "181\n",
      "train loss:0.572217081378\n",
      "182\n",
      "train loss:0.559327313973\n",
      "183\n",
      "train loss:0.58316974998\n",
      "184\n",
      "train loss:0.704286490226\n",
      "185\n",
      "train loss:0.493716430399\n",
      "186\n",
      "train loss:0.520343652772\n",
      "187\n",
      "train loss:0.590044290058\n",
      "188\n",
      "train loss:0.5340773441\n",
      "189\n",
      "train loss:0.612149257584\n",
      "190\n",
      "train loss:0.611437153996\n",
      "191\n",
      "train loss:0.575380521369\n",
      "192\n",
      "train loss:0.542990115893\n",
      "193\n",
      "train loss:0.665970476894\n",
      "194\n",
      "train loss:0.556086167616\n",
      "195\n",
      "train loss:0.557494932165\n",
      "196\n",
      "train loss:0.464089620406\n",
      "197\n",
      "train loss:0.694850717342\n",
      "198\n",
      "train loss:0.603252333267\n",
      "199\n",
      "train loss:0.722054796646\n",
      "200\n",
      "train loss:0.545652195736\n",
      "201\n",
      "train loss:0.568379585801\n",
      "202\n",
      "train loss:0.54742076804\n",
      "203\n",
      "train loss:0.613825294686\n",
      "204\n",
      "train loss:0.539326159441\n",
      "205\n",
      "train loss:0.870108319847\n",
      "206\n",
      "train loss:0.494000416547\n",
      "207\n",
      "train loss:0.638886822782\n",
      "208\n",
      "train loss:0.593533192777\n",
      "209\n",
      "train loss:0.507328039732\n",
      "210\n",
      "train loss:0.695155252767\n",
      "211\n",
      "train loss:0.5930331537\n",
      "212\n",
      "train loss:0.514051136575\n",
      "213\n",
      "train loss:0.489554020763\n",
      "214\n",
      "train loss:0.583599336818\n",
      "215\n",
      "train loss:0.533686531805\n",
      "216\n",
      "train loss:0.635933991861\n",
      "217\n",
      "train loss:0.566484817153\n",
      "218\n",
      "train loss:0.586495520947\n",
      "219\n",
      "train loss:0.55737500125\n",
      "220\n",
      "train loss:0.56495521445\n",
      "221\n",
      "train loss:0.609805290192\n",
      "222\n",
      "train loss:0.614384277933\n",
      "223\n",
      "train loss:0.620364940843\n",
      "224\n",
      "train loss:0.526064056368\n",
      "225\n",
      "train loss:0.551279080492\n",
      "226\n",
      "train loss:0.534173470042\n",
      "227\n",
      "train loss:0.591630461736\n",
      "228\n",
      "train loss:0.559685778442\n",
      "229\n",
      "train loss:0.610866978916\n",
      "230\n",
      "train loss:0.556214120431\n",
      "231\n",
      "train loss:0.654671342859\n",
      "232\n",
      "train loss:0.641117615713\n",
      "233\n",
      "train loss:0.664835433014\n",
      "234\n",
      "train loss:0.547447271479\n",
      "235\n",
      "train loss:0.449648163007\n",
      "236\n",
      "train loss:0.624312042944\n",
      "237\n",
      "train loss:0.624925088652\n",
      "238\n",
      "train loss:0.565741794441\n",
      "239\n",
      "train loss:0.533553815292\n",
      "240\n",
      "train loss:0.584811119591\n",
      "241\n",
      "train loss:0.569613202557\n",
      "242\n",
      "train loss:0.74224951603\n",
      "243\n",
      "train loss:0.620937537478\n",
      "244\n",
      "train loss:0.57663221693\n",
      "245\n",
      "train loss:0.620411577766\n",
      "246\n",
      "train loss:0.564848022527\n",
      "247\n",
      "train loss:0.558051484919\n",
      "248\n",
      "train loss:0.593992728238\n",
      "249\n",
      "train loss:0.601348303409\n",
      "250\n",
      "train loss:0.606296712109\n",
      "251\n",
      "train loss:0.543824573383\n",
      "252\n",
      "train loss:0.681490934674\n",
      "253\n",
      "train loss:0.511715706741\n",
      "254\n",
      "train loss:0.624989131826\n",
      "255\n",
      "train loss:0.593001609596\n",
      "256\n",
      "train loss:0.613005823809\n",
      "257\n",
      "train loss:0.622447386298\n",
      "258\n",
      "train loss:0.501510259209\n",
      "259\n",
      "train loss:0.577272950594\n",
      "260\n",
      "train loss:0.728430512915\n",
      "261\n",
      "train loss:0.612990967339\n",
      "262\n",
      "train loss:0.59190346728\n",
      "263\n",
      "train loss:0.576701330067\n",
      "264\n",
      "train loss:0.648205320673\n",
      "265\n",
      "train loss:0.577230212042\n",
      "266\n",
      "train loss:0.550261277925\n",
      "0\n",
      "train loss:0.539230761178\n",
      "4th epoch end: Saved Network Parameters!\n",
      "=== epoch:4, train acc:0.995, test acc:0.987 ===\n",
      "1\n",
      "train loss:0.686166983182\n",
      "2\n",
      "train loss:0.668585279683\n",
      "3\n",
      "train loss:0.539720682355\n",
      "4\n",
      "train loss:0.605318269548\n",
      "5\n",
      "train loss:0.446390102664\n",
      "6\n",
      "train loss:0.458916167766\n",
      "7\n",
      "train loss:0.530243899317\n",
      "8\n",
      "train loss:0.600256551268\n",
      "9\n",
      "train loss:0.631838384323\n",
      "10\n",
      "train loss:0.571248359403\n",
      "11\n",
      "train loss:0.65628834165\n",
      "12\n",
      "train loss:0.526086610341\n",
      "13\n",
      "train loss:0.570221232396\n",
      "14\n",
      "train loss:0.56649761948\n",
      "15\n",
      "train loss:0.506989240552\n",
      "16\n",
      "train loss:0.483919606528\n",
      "17\n",
      "train loss:0.544982130847\n",
      "18\n",
      "train loss:0.58475018676\n",
      "19\n",
      "train loss:0.593217933939\n",
      "20\n",
      "train loss:0.537854930435\n",
      "21\n",
      "train loss:0.5314212287\n",
      "22\n",
      "train loss:0.524465449439\n",
      "23\n",
      "train loss:0.526609406408\n",
      "24\n",
      "train loss:0.434168268619\n",
      "25\n",
      "train loss:0.596062897398\n",
      "26\n",
      "train loss:0.656018269468\n",
      "27\n",
      "train loss:0.558919801812\n",
      "28\n",
      "train loss:0.541844697505\n",
      "29\n",
      "train loss:0.544179025664\n",
      "30\n",
      "train loss:0.448743318203\n",
      "31\n",
      "train loss:0.50258383416\n",
      "32\n",
      "train loss:0.599610198305\n",
      "33\n",
      "train loss:0.565636216331\n",
      "34\n",
      "train loss:0.672720501673\n",
      "35\n",
      "train loss:0.633170744183\n",
      "36\n",
      "train loss:0.706526709122\n",
      "37\n",
      "train loss:0.590913085881\n",
      "38\n",
      "train loss:0.523283661852\n",
      "39\n",
      "train loss:0.603693087691\n",
      "40\n",
      "train loss:0.583454832559\n",
      "41\n",
      "train loss:0.510658837083\n",
      "42\n",
      "train loss:0.551084287469\n",
      "43\n",
      "train loss:0.476975937287\n",
      "44\n",
      "train loss:0.56508760914\n",
      "45\n",
      "train loss:0.528960419147\n",
      "46\n",
      "train loss:0.750797726376\n",
      "47\n",
      "train loss:0.605885763141\n",
      "48\n",
      "train loss:0.547714788627\n",
      "49\n",
      "train loss:0.679305366692\n",
      "50\n",
      "train loss:0.63881915579\n",
      "51\n",
      "train loss:0.678759345352\n",
      "52\n",
      "train loss:0.518784933875\n",
      "53\n",
      "train loss:0.525639496381\n",
      "54\n",
      "train loss:0.638465910036\n",
      "55\n",
      "train loss:0.433601738558\n",
      "56\n",
      "train loss:0.542565112907\n",
      "57\n",
      "train loss:0.534616793276\n",
      "58\n",
      "train loss:0.606764397574\n",
      "59\n",
      "train loss:0.640583223896\n",
      "60\n",
      "train loss:0.605282814775\n",
      "61\n",
      "train loss:0.694122568147\n",
      "62\n",
      "train loss:0.648822405147\n",
      "63\n",
      "train loss:0.589704566973\n",
      "64\n",
      "train loss:0.620444277465\n",
      "65\n",
      "train loss:0.589762822133\n",
      "66\n",
      "train loss:0.504689011741\n",
      "67\n",
      "train loss:0.576371683336\n",
      "68\n",
      "train loss:0.54974316066\n",
      "69\n",
      "train loss:0.536171028717\n",
      "70\n",
      "train loss:0.653535364506\n",
      "71\n",
      "train loss:0.559810727213\n",
      "72\n",
      "train loss:0.545576721614\n",
      "73\n",
      "train loss:0.508546978339\n",
      "74\n",
      "train loss:0.649623012595\n",
      "75\n",
      "train loss:0.589814517491\n",
      "76\n",
      "train loss:0.615525324325\n",
      "77\n",
      "train loss:0.492402323059\n",
      "78\n",
      "train loss:0.535161873279\n",
      "79\n",
      "train loss:0.656015320925\n",
      "80\n",
      "train loss:0.791020185039\n",
      "81\n",
      "train loss:0.594108223598\n",
      "82\n",
      "train loss:0.503196121151\n",
      "83\n",
      "train loss:0.655436378793\n",
      "84\n",
      "train loss:0.569059727522\n",
      "85\n",
      "train loss:0.502748909035\n",
      "86\n",
      "train loss:0.570071579145\n",
      "87\n",
      "train loss:0.562140096542\n",
      "88\n",
      "train loss:0.514985918677\n",
      "89\n",
      "train loss:0.597221236994\n",
      "90\n",
      "train loss:0.54871525314\n",
      "91\n",
      "train loss:0.594502977464\n",
      "92\n",
      "train loss:0.556563765018\n",
      "93\n",
      "train loss:0.551363576528\n",
      "94\n",
      "train loss:0.536355001669\n",
      "95\n",
      "train loss:0.599092594362\n",
      "96\n",
      "train loss:0.60225539013\n",
      "97\n",
      "train loss:0.598130250465\n",
      "98\n",
      "train loss:0.585762981042\n",
      "99\n",
      "train loss:0.51580481599\n",
      "100\n",
      "train loss:0.696783351502\n",
      "101\n",
      "train loss:0.65484437331\n",
      "102\n",
      "train loss:0.674456576944\n",
      "103\n",
      "train loss:0.651230686494\n",
      "104\n",
      "train loss:0.541919828495\n",
      "105\n",
      "train loss:0.617935972935\n",
      "106\n",
      "train loss:0.612648850896\n",
      "107\n",
      "train loss:0.643885206824\n",
      "108\n",
      "train loss:0.582442711384\n",
      "109\n",
      "train loss:0.587825534478\n",
      "110\n",
      "train loss:0.654197990608\n",
      "111\n",
      "train loss:0.764135570137\n",
      "112\n",
      "train loss:0.608316635184\n",
      "113\n",
      "train loss:0.523333645094\n",
      "114\n",
      "train loss:0.468574481116\n",
      "115\n",
      "train loss:0.585649314689\n",
      "116\n",
      "train loss:0.63291866097\n",
      "117\n",
      "train loss:0.564674976307\n",
      "118\n",
      "train loss:0.645378029471\n",
      "119\n",
      "train loss:0.633840035903\n",
      "120\n",
      "train loss:0.474587118091\n",
      "121\n",
      "train loss:0.603494075877\n",
      "122\n",
      "train loss:0.664911665133\n",
      "123\n",
      "train loss:0.581248805347\n",
      "124\n",
      "train loss:0.530154503569\n",
      "125\n",
      "train loss:0.661296632541\n",
      "126\n",
      "train loss:0.43666383418\n",
      "127\n",
      "train loss:0.572088141095\n",
      "128\n",
      "train loss:0.569529276235\n",
      "129\n",
      "train loss:0.573384635553\n",
      "130\n",
      "train loss:0.564124440066\n",
      "131\n",
      "train loss:0.561163883923\n",
      "132\n",
      "train loss:0.556582711487\n",
      "133\n",
      "train loss:0.486714170097\n",
      "134\n",
      "train loss:0.560624797982\n",
      "135\n",
      "train loss:0.704067614812\n",
      "136\n",
      "train loss:0.435671260845\n",
      "137\n",
      "train loss:0.559686650814\n",
      "138\n",
      "train loss:0.535260206952\n",
      "139\n",
      "train loss:0.495965218147\n",
      "140\n",
      "train loss:0.50753475818\n",
      "141\n",
      "train loss:0.536761994849\n",
      "142\n",
      "train loss:0.593271573893\n",
      "143\n",
      "train loss:0.661199958308\n",
      "144\n",
      "train loss:0.625353246382\n",
      "145\n",
      "train loss:0.526457173883\n",
      "146\n",
      "train loss:0.548658752398\n",
      "147\n",
      "train loss:0.548265902102\n",
      "148\n",
      "train loss:0.547108750684\n",
      "149\n",
      "train loss:0.537142672331\n",
      "150\n",
      "train loss:0.439526801301\n",
      "151\n",
      "train loss:0.591964683432\n",
      "152\n",
      "train loss:0.546406602501\n",
      "153\n",
      "train loss:0.470026168547\n",
      "154\n",
      "train loss:0.630283863807\n",
      "155\n",
      "train loss:0.688536861881\n",
      "156\n",
      "train loss:0.574040692448\n",
      "157\n",
      "train loss:0.664328313082\n",
      "158\n",
      "train loss:0.483065165821\n",
      "159\n",
      "train loss:0.558262629766\n",
      "160\n",
      "train loss:0.473826503627\n",
      "161\n",
      "train loss:0.541373386146\n",
      "162\n",
      "train loss:0.609938241765\n",
      "163\n",
      "train loss:0.57631497052\n",
      "164\n",
      "train loss:0.572698991187\n",
      "165\n",
      "train loss:0.50386640951\n",
      "166\n",
      "train loss:0.542318689164\n",
      "167\n",
      "train loss:0.643243146605\n",
      "168\n",
      "train loss:0.658586236167\n",
      "169\n",
      "train loss:0.491447900453\n",
      "170\n",
      "train loss:0.473363966944\n",
      "171\n",
      "train loss:0.574842451448\n",
      "172\n",
      "train loss:0.523449775472\n",
      "173\n",
      "train loss:0.489889395685\n",
      "174\n",
      "train loss:0.529074069038\n",
      "175\n",
      "train loss:0.528780967697\n",
      "176\n",
      "train loss:0.476665811393\n",
      "177\n",
      "train loss:0.524391639067\n",
      "178\n",
      "train loss:0.613205023118\n",
      "179\n",
      "train loss:0.547251951987\n",
      "180\n",
      "train loss:0.564695526424\n",
      "181\n",
      "train loss:0.50182827432\n",
      "182\n",
      "train loss:0.52384742432\n",
      "183\n",
      "train loss:0.654951217796\n",
      "184\n",
      "train loss:0.535345497744\n",
      "185\n",
      "train loss:0.584859265932\n",
      "186\n",
      "train loss:0.695457623369\n",
      "187\n",
      "train loss:0.567734867345\n",
      "188\n",
      "train loss:0.579010314328\n",
      "189\n",
      "train loss:0.500121546448\n",
      "190\n",
      "train loss:0.520920366978\n",
      "191\n",
      "train loss:0.527487713143\n",
      "192\n",
      "train loss:0.620030694654\n",
      "193\n",
      "train loss:0.538940595096\n",
      "194\n",
      "train loss:0.605601161831\n",
      "195\n",
      "train loss:0.515119086724\n",
      "196\n",
      "train loss:0.494326649353\n",
      "197\n",
      "train loss:0.583615405635\n",
      "198\n",
      "train loss:0.550851151108\n",
      "199\n",
      "train loss:0.541773022925\n",
      "200\n",
      "train loss:0.558412109\n",
      "201\n",
      "train loss:0.645383480617\n",
      "202\n",
      "train loss:0.500699125088\n",
      "203\n",
      "train loss:0.682367500154\n",
      "204\n",
      "train loss:0.530843837512\n",
      "205\n",
      "train loss:0.569862427196\n",
      "206\n",
      "train loss:0.620574665287\n",
      "207\n",
      "train loss:0.592581988763\n",
      "208\n",
      "train loss:0.502469761196\n",
      "209\n",
      "train loss:0.636191700749\n",
      "210\n",
      "train loss:0.593308639133\n",
      "211\n",
      "train loss:0.468381537503\n",
      "212\n",
      "train loss:0.577419248607\n",
      "213\n",
      "train loss:0.558660598439\n",
      "214\n",
      "train loss:0.60708256828\n",
      "215\n",
      "train loss:0.619249148099\n",
      "216\n",
      "train loss:0.697571428438\n",
      "217\n",
      "train loss:0.584927331587\n",
      "218\n",
      "train loss:0.52147455272\n",
      "219\n",
      "train loss:0.653871449647\n",
      "220\n",
      "train loss:0.570793984355\n",
      "221\n",
      "train loss:0.6227281473\n",
      "222\n",
      "train loss:0.56515662774\n",
      "223\n",
      "train loss:0.537637769977\n",
      "224\n",
      "train loss:0.55610141596\n",
      "225\n",
      "train loss:0.560937599024\n",
      "226\n",
      "train loss:0.588656209017\n",
      "227\n",
      "train loss:0.641922380629\n",
      "228\n",
      "train loss:0.531815044829\n",
      "229\n",
      "train loss:0.590562306775\n",
      "230\n",
      "train loss:0.499225908564\n",
      "231\n",
      "train loss:0.54416069941\n",
      "232\n",
      "train loss:0.444673243021\n",
      "233\n",
      "train loss:0.608241433064\n",
      "234\n",
      "train loss:0.543425575369\n",
      "235\n",
      "train loss:0.49888110934\n",
      "236\n",
      "train loss:0.480703694702\n",
      "237\n",
      "train loss:0.565430957062\n",
      "238\n",
      "train loss:0.537054077861\n",
      "239\n",
      "train loss:0.57476340143\n",
      "240\n",
      "train loss:0.532145672202\n",
      "241\n",
      "train loss:0.682599718455\n",
      "242\n",
      "train loss:0.586765411242\n",
      "243\n",
      "train loss:0.701150236511\n",
      "244\n",
      "train loss:0.681396117814\n",
      "245\n",
      "train loss:0.549317734521\n",
      "246\n",
      "train loss:0.585005989082\n",
      "247\n",
      "train loss:0.577761933348\n",
      "248\n",
      "train loss:0.552470128721\n",
      "249\n",
      "train loss:0.660074887354\n",
      "250\n",
      "train loss:0.582567964547\n",
      "251\n",
      "train loss:0.558861471635\n",
      "252\n",
      "train loss:0.493999549174\n",
      "253\n",
      "train loss:0.653754030798\n",
      "254\n",
      "train loss:0.4655671986\n",
      "255\n",
      "train loss:0.588077329964\n",
      "256\n",
      "train loss:0.574071252376\n",
      "257\n",
      "train loss:0.646760065485\n",
      "258\n",
      "train loss:0.537045018017\n",
      "259\n",
      "train loss:0.542254450685\n",
      "260\n",
      "train loss:0.559568850613\n",
      "261\n",
      "train loss:0.530211933353\n",
      "262\n",
      "train loss:0.568573721607\n",
      "263\n",
      "train loss:0.605707342461\n",
      "264\n",
      "train loss:0.488205873929\n",
      "265\n",
      "train loss:0.413312717558\n",
      "266\n",
      "train loss:0.489890436736\n",
      "0\n",
      "train loss:0.645862640962\n",
      "5th epoch end: Saved Network Parameters!\n",
      "=== epoch:5, train acc:0.995, test acc:0.985 ===\n",
      "1\n",
      "train loss:0.566686206448\n",
      "2\n",
      "train loss:0.61565474307\n",
      "3\n",
      "train loss:0.444251200671\n",
      "4\n",
      "train loss:0.483116289654\n",
      "5\n",
      "train loss:0.555242745931\n",
      "6\n",
      "train loss:0.549930856608\n",
      "7\n",
      "train loss:0.546445437829\n",
      "8\n",
      "train loss:0.532329581749\n",
      "9\n",
      "train loss:0.545798366527\n",
      "10\n",
      "train loss:0.639046892821\n",
      "11\n",
      "train loss:0.548178109365\n",
      "12\n",
      "train loss:0.49271804355\n",
      "13\n",
      "train loss:0.475729126185\n",
      "14\n",
      "train loss:0.442179198488\n",
      "15\n",
      "train loss:0.515932378562\n",
      "16\n",
      "train loss:0.559840803979\n",
      "17\n",
      "train loss:0.498010816957\n",
      "18\n",
      "train loss:0.523901280286\n",
      "19\n",
      "train loss:0.499843911007\n",
      "20\n",
      "train loss:0.46386924579\n",
      "21\n",
      "train loss:0.572720854947\n",
      "22\n",
      "train loss:0.64826853996\n",
      "23\n",
      "train loss:0.592114450253\n",
      "24\n",
      "train loss:0.506520813006\n",
      "25\n",
      "train loss:0.585112820106\n",
      "26\n",
      "train loss:0.556346582101\n",
      "27\n",
      "train loss:0.558709348511\n",
      "28\n",
      "train loss:0.585382120371\n",
      "29\n",
      "train loss:0.585418245335\n",
      "30\n",
      "train loss:0.465111201494\n",
      "31\n",
      "train loss:0.52511159463\n",
      "32\n",
      "train loss:0.460336877035\n",
      "33\n",
      "train loss:0.471851734042\n",
      "34\n",
      "train loss:0.632660846839\n",
      "35\n",
      "train loss:0.516147891634\n",
      "36\n",
      "train loss:0.499827201482\n",
      "37\n",
      "train loss:0.638776439646\n",
      "38\n",
      "train loss:0.547579147455\n",
      "39\n",
      "train loss:0.611769620786\n",
      "40\n",
      "train loss:0.494468834366\n",
      "41\n",
      "train loss:0.547483073006\n",
      "42\n",
      "train loss:0.541216727043\n",
      "43\n",
      "train loss:0.507439908516\n",
      "44\n",
      "train loss:0.624047259062\n",
      "45\n",
      "train loss:0.545838152833\n",
      "46\n",
      "train loss:0.493141560758\n",
      "47\n",
      "train loss:0.562341143278\n",
      "48\n",
      "train loss:0.571326222274\n",
      "49\n",
      "train loss:0.510813426564\n",
      "50\n",
      "train loss:0.438053183573\n",
      "51\n",
      "train loss:0.458001044143\n",
      "52\n",
      "train loss:0.485020239649\n",
      "53\n",
      "train loss:0.512555994828\n",
      "54\n",
      "train loss:0.579163656497\n",
      "55\n",
      "train loss:0.494708796191\n",
      "56\n",
      "train loss:0.507793942807\n",
      "57\n",
      "train loss:0.570989892708\n",
      "58\n",
      "train loss:0.585873092928\n",
      "59\n",
      "train loss:0.534107436562\n",
      "60\n",
      "train loss:0.560650990867\n",
      "61\n",
      "train loss:0.515728071245\n",
      "62\n",
      "train loss:0.555368091198\n",
      "63\n",
      "train loss:0.502997064841\n",
      "64\n",
      "train loss:0.563899752001\n",
      "65\n",
      "train loss:0.572999817239\n",
      "66\n",
      "train loss:0.487992502856\n",
      "67\n",
      "train loss:0.521038081833\n",
      "68\n",
      "train loss:0.485305580168\n",
      "69\n",
      "train loss:0.554395644855\n",
      "70\n",
      "train loss:0.709842098271\n",
      "71\n",
      "train loss:0.570781313621\n",
      "72\n",
      "train loss:0.505428774654\n",
      "73\n",
      "train loss:0.661198838636\n",
      "74\n",
      "train loss:0.564857932533\n",
      "75\n",
      "train loss:0.541430049527\n",
      "76\n",
      "train loss:0.509174874893\n",
      "77\n",
      "train loss:0.544559914245\n",
      "78\n",
      "train loss:0.542431757196\n",
      "79\n",
      "train loss:0.649781130982\n",
      "80\n",
      "train loss:0.468098152642\n",
      "81\n",
      "train loss:0.426013704755\n",
      "82\n",
      "train loss:0.443684444771\n",
      "83\n",
      "train loss:0.478289306205\n",
      "84\n",
      "train loss:0.538631721081\n",
      "85\n",
      "train loss:0.576473146405\n",
      "86\n",
      "train loss:0.494422088803\n",
      "87\n",
      "train loss:0.517178026733\n",
      "88\n",
      "train loss:0.565108476087\n",
      "89\n",
      "train loss:0.534426788282\n",
      "90\n",
      "train loss:0.570284790908\n",
      "91\n",
      "train loss:0.459929398671\n",
      "92\n",
      "train loss:0.515740712692\n",
      "93\n",
      "train loss:0.500336384237\n",
      "94\n",
      "train loss:0.598130118527\n",
      "95\n",
      "train loss:0.647327832716\n",
      "96\n",
      "train loss:0.637096832847\n",
      "97\n",
      "train loss:0.543576651282\n",
      "98\n",
      "train loss:0.564814615465\n",
      "99\n",
      "train loss:0.634429879041\n",
      "100\n",
      "train loss:0.477426984516\n",
      "101\n",
      "train loss:0.482910900985\n",
      "102\n",
      "train loss:0.537166545531\n",
      "103\n",
      "train loss:0.555745358619\n",
      "104\n",
      "train loss:0.515930503385\n",
      "105\n",
      "train loss:0.553141416775\n",
      "106\n",
      "train loss:0.464615469228\n",
      "107\n",
      "train loss:0.605155136067\n",
      "108\n",
      "train loss:0.468901426134\n",
      "109\n",
      "train loss:0.522479481531\n",
      "110\n",
      "train loss:0.513882279558\n",
      "111\n",
      "train loss:0.570440056649\n",
      "112\n",
      "train loss:0.596024651266\n",
      "113\n",
      "train loss:0.626776668407\n",
      "114\n",
      "train loss:0.498651357109\n",
      "115\n",
      "train loss:0.599155831598\n",
      "116\n",
      "train loss:0.552711470026\n",
      "117\n",
      "train loss:0.635567873098\n",
      "118\n",
      "train loss:0.574049222242\n",
      "119\n",
      "train loss:0.584537191279\n",
      "120\n",
      "train loss:0.506589493207\n",
      "121\n",
      "train loss:0.619518358737\n",
      "122\n",
      "train loss:0.649996647757\n",
      "123\n",
      "train loss:0.600414481359\n",
      "124\n",
      "train loss:0.658420744321\n",
      "125\n",
      "train loss:0.515686522761\n",
      "126\n",
      "train loss:0.53877447814\n",
      "127\n",
      "train loss:0.702346272658\n",
      "128\n",
      "train loss:0.593526379234\n",
      "129\n",
      "train loss:0.414199006125\n",
      "130\n",
      "train loss:0.531597926668\n",
      "131\n",
      "train loss:0.514219757103\n",
      "132\n",
      "train loss:0.539752782191\n",
      "133\n",
      "train loss:0.509991299775\n",
      "134\n",
      "train loss:0.598989884079\n",
      "135\n",
      "train loss:0.415601020917\n",
      "136\n",
      "train loss:0.569872054758\n",
      "137\n",
      "train loss:0.440741674582\n",
      "138\n",
      "train loss:0.45018030306\n",
      "139\n",
      "train loss:0.494505362162\n",
      "140\n",
      "train loss:0.591665987388\n",
      "141\n",
      "train loss:0.494526255646\n",
      "142\n",
      "train loss:0.592496646625\n",
      "143\n",
      "train loss:0.495629922391\n",
      "144\n",
      "train loss:0.544826707291\n",
      "145\n",
      "train loss:0.58011463591\n",
      "146\n",
      "train loss:0.523939524389\n",
      "147\n",
      "train loss:0.471633020782\n",
      "148\n",
      "train loss:0.474785000037\n",
      "149\n",
      "train loss:0.576419653116\n",
      "150\n",
      "train loss:0.611097761131\n",
      "151\n",
      "train loss:0.622658917832\n",
      "152\n",
      "train loss:0.596164347788\n",
      "153\n",
      "train loss:0.438825068373\n",
      "154\n",
      "train loss:0.56225356552\n",
      "155\n",
      "train loss:0.455992487533\n",
      "156\n",
      "train loss:0.611050374043\n",
      "157\n",
      "train loss:0.55546197009\n",
      "158\n",
      "train loss:0.719242273606\n",
      "159\n",
      "train loss:0.505299224916\n",
      "160\n",
      "train loss:0.477348237176\n",
      "161\n",
      "train loss:0.552819571288\n",
      "162\n",
      "train loss:0.514789301378\n",
      "163\n",
      "train loss:0.663431282178\n",
      "164\n",
      "train loss:0.574505799715\n",
      "165\n",
      "train loss:0.605353238943\n",
      "166\n",
      "train loss:0.49484289833\n",
      "167\n",
      "train loss:0.553744018566\n",
      "168\n",
      "train loss:0.580274219543\n",
      "169\n",
      "train loss:0.638233467028\n",
      "170\n",
      "train loss:0.562402376398\n",
      "171\n",
      "train loss:0.531731585303\n",
      "172\n",
      "train loss:0.45249949651\n",
      "173\n",
      "train loss:0.490155296325\n",
      "174\n",
      "train loss:0.534791643492\n",
      "175\n",
      "train loss:0.580562160262\n",
      "176\n",
      "train loss:0.474796587608\n",
      "177\n",
      "train loss:0.485278699154\n",
      "178\n",
      "train loss:0.410271299931\n",
      "179\n",
      "train loss:0.6199354772\n",
      "180\n",
      "train loss:0.565921110689\n",
      "181\n",
      "train loss:0.541050579429\n",
      "182\n",
      "train loss:0.484222421293\n",
      "183\n",
      "train loss:0.524342101605\n",
      "184\n",
      "train loss:0.523971487623\n",
      "185\n",
      "train loss:0.528706199235\n",
      "186\n",
      "train loss:0.603952222026\n",
      "187\n",
      "train loss:0.494644586983\n",
      "188\n",
      "train loss:0.642738876376\n",
      "189\n",
      "train loss:0.528798084214\n",
      "190\n",
      "train loss:0.537317428846\n",
      "191\n",
      "train loss:0.511699847883\n",
      "192\n",
      "train loss:0.475862702091\n",
      "193\n",
      "train loss:0.624740189014\n",
      "194\n",
      "train loss:0.504963319769\n",
      "195\n",
      "train loss:0.642968861617\n",
      "196\n",
      "train loss:0.580929692583\n",
      "197\n",
      "train loss:0.495047186962\n",
      "198\n",
      "train loss:0.475446125289\n",
      "199\n",
      "train loss:0.491074324163\n",
      "200\n",
      "train loss:0.624131702266\n",
      "201\n",
      "train loss:0.45038031556\n",
      "202\n",
      "train loss:0.47438844373\n",
      "203\n",
      "train loss:0.429615507474\n",
      "204\n",
      "train loss:0.557509826552\n",
      "205\n",
      "train loss:0.545698538815\n",
      "206\n",
      "train loss:0.594409547849\n",
      "207\n",
      "train loss:0.618272995177\n",
      "208\n",
      "train loss:0.397731012428\n",
      "209\n",
      "train loss:0.533243370727\n",
      "210\n",
      "train loss:0.501260766122\n",
      "211\n",
      "train loss:0.597148240612\n",
      "212\n",
      "train loss:0.515425735768\n",
      "213\n",
      "train loss:0.525392360751\n",
      "214\n",
      "train loss:0.483112541832\n",
      "215\n",
      "train loss:0.654250527314\n",
      "216\n",
      "train loss:0.465726387987\n",
      "217\n",
      "train loss:0.593095564565\n",
      "218\n",
      "train loss:0.569211347644\n",
      "219\n",
      "train loss:0.561888149575\n",
      "220\n",
      "train loss:0.497959920603\n",
      "221\n",
      "train loss:0.556046622492\n",
      "222\n",
      "train loss:0.47845161685\n",
      "223\n",
      "train loss:0.554638432327\n",
      "224\n",
      "train loss:0.547571938361\n",
      "225\n",
      "train loss:0.540103756932\n",
      "226\n",
      "train loss:0.599243925269\n",
      "227\n",
      "train loss:0.55767871578\n",
      "228\n",
      "train loss:0.557209672046\n",
      "229\n",
      "train loss:0.519637967731\n",
      "230\n",
      "train loss:0.596608495325\n",
      "231\n",
      "train loss:0.390895891968\n",
      "232\n",
      "train loss:0.433656105219\n",
      "233\n",
      "train loss:0.455964238469\n",
      "234\n",
      "train loss:0.548739327438\n",
      "235\n",
      "train loss:0.548077359388\n",
      "236\n",
      "train loss:0.502075775624\n",
      "237\n",
      "train loss:0.590928910842\n",
      "238\n",
      "train loss:0.475003883159\n",
      "239\n",
      "train loss:0.52620900699\n",
      "240\n",
      "train loss:0.485905054497\n",
      "241\n",
      "train loss:0.480889898409\n",
      "242\n",
      "train loss:0.745692972503\n",
      "243\n",
      "train loss:0.540763305739\n",
      "244\n",
      "train loss:0.525043103697\n",
      "245\n",
      "train loss:0.507298959652\n",
      "246\n",
      "train loss:0.533255127454\n",
      "247\n",
      "train loss:0.561701055734\n",
      "248\n",
      "train loss:0.509669511922\n",
      "249\n",
      "train loss:0.477417042914\n",
      "250\n",
      "train loss:0.496337182035\n",
      "251\n",
      "train loss:0.678559340615\n",
      "252\n",
      "train loss:0.506389097296\n",
      "253\n",
      "train loss:0.429999359159\n",
      "254\n",
      "train loss:0.558823641351\n",
      "255\n",
      "train loss:0.489977692811\n",
      "256\n",
      "train loss:0.47596543055\n",
      "257\n",
      "train loss:0.523918902797\n",
      "258\n",
      "train loss:0.598898120676\n",
      "259\n",
      "train loss:0.463977379795\n",
      "260\n",
      "train loss:0.491868956229\n",
      "261\n",
      "train loss:0.523008555566\n",
      "262\n",
      "train loss:0.493117509152\n",
      "263\n",
      "train loss:0.693586779326\n",
      "264\n",
      "train loss:0.445832368825\n",
      "265\n",
      "train loss:0.488377177705\n",
      "266\n",
      "train loss:0.555281550814\n",
      "0\n",
      "train loss:0.433035373547\n",
      "6th epoch end: Saved Network Parameters!\n",
      "=== epoch:6, train acc:0.999, test acc:0.986 ===\n",
      "1\n",
      "train loss:0.470367413335\n",
      "2\n",
      "train loss:0.573479664793\n",
      "3\n",
      "train loss:0.483117644847\n",
      "4\n",
      "train loss:0.464183758347\n",
      "5\n",
      "train loss:0.525340701027\n",
      "6\n",
      "train loss:0.588001135392\n",
      "7\n",
      "train loss:0.644903636894\n",
      "8\n",
      "train loss:0.534685564457\n",
      "9\n",
      "train loss:0.446182241652\n",
      "10\n",
      "train loss:0.553497772923\n",
      "11\n",
      "train loss:0.524845512993\n",
      "12\n",
      "train loss:0.64479752695\n",
      "13\n",
      "train loss:0.548227946372\n",
      "14\n",
      "train loss:0.461716597027\n",
      "15\n",
      "train loss:0.551379036036\n",
      "16\n",
      "train loss:0.645872453432\n",
      "17\n",
      "train loss:0.593211482269\n",
      "18\n",
      "train loss:0.52023676538\n",
      "19\n",
      "train loss:0.661673519378\n",
      "20\n",
      "train loss:0.539071607429\n",
      "21\n",
      "train loss:0.59986337777\n",
      "22\n",
      "train loss:0.588644571174\n",
      "23\n",
      "train loss:0.531431603551\n",
      "24\n",
      "train loss:0.58788626837\n",
      "25\n",
      "train loss:0.527525107335\n",
      "26\n",
      "train loss:0.467773004482\n",
      "27\n",
      "train loss:0.434905221463\n",
      "28\n",
      "train loss:0.514302664425\n",
      "29\n",
      "train loss:0.527644257882\n",
      "30\n",
      "train loss:0.569676792788\n",
      "31\n",
      "train loss:0.610655778144\n",
      "32\n",
      "train loss:0.535871237562\n",
      "33\n",
      "train loss:0.601188274962\n",
      "34\n",
      "train loss:0.494979842318\n",
      "35\n",
      "train loss:0.504143195134\n",
      "36\n",
      "train loss:0.516660442859\n",
      "37\n",
      "train loss:0.53446935621\n",
      "38\n",
      "train loss:0.523592114889\n",
      "39\n",
      "train loss:0.627245094049\n",
      "40\n",
      "train loss:0.425288078445\n",
      "41\n",
      "train loss:0.525258209992\n",
      "42\n",
      "train loss:0.486079065353\n",
      "43\n",
      "train loss:0.559377776576\n",
      "44\n",
      "train loss:0.651249074233\n",
      "45\n",
      "train loss:0.544538293733\n",
      "46\n",
      "train loss:0.545755736529\n",
      "47\n",
      "train loss:0.578682492201\n",
      "48\n",
      "train loss:0.596931072481\n",
      "49\n",
      "train loss:0.49957194977\n",
      "50\n",
      "train loss:0.491549648327\n",
      "51\n",
      "train loss:0.499366076691\n",
      "52\n",
      "train loss:0.479939831593\n",
      "53\n",
      "train loss:0.578989135599\n",
      "54\n",
      "train loss:0.616449362595\n",
      "55\n",
      "train loss:0.53501412131\n",
      "56\n",
      "train loss:0.440820734079\n",
      "57\n",
      "train loss:0.551011946677\n",
      "58\n",
      "train loss:0.530042287064\n",
      "59\n",
      "train loss:0.540067907569\n",
      "60\n",
      "train loss:0.446542261405\n",
      "61\n",
      "train loss:0.457915934819\n",
      "62\n",
      "train loss:0.521185132069\n",
      "63\n",
      "train loss:0.605199192167\n",
      "64\n",
      "train loss:0.465568653436\n",
      "65\n",
      "train loss:0.496616724932\n",
      "66\n",
      "train loss:0.527572925265\n",
      "67\n",
      "train loss:0.427042666244\n",
      "68\n",
      "train loss:0.454909060056\n",
      "69\n",
      "train loss:0.491023221393\n",
      "70\n",
      "train loss:0.60032116187\n",
      "71\n",
      "train loss:0.456784233351\n",
      "72\n",
      "train loss:0.423842617894\n",
      "73\n",
      "train loss:0.533911888891\n",
      "74\n",
      "train loss:0.568538883234\n",
      "75\n",
      "train loss:0.511951061396\n",
      "76\n",
      "train loss:0.436477852224\n",
      "77\n",
      "train loss:0.531264816852\n",
      "78\n",
      "train loss:0.479862814449\n",
      "79\n",
      "train loss:0.584227832286\n",
      "80\n",
      "train loss:0.497607696287\n",
      "81\n",
      "train loss:0.506116565894\n",
      "82\n",
      "train loss:0.495631384149\n",
      "83\n",
      "train loss:0.456363372872\n",
      "84\n",
      "train loss:0.523739668896\n",
      "85\n",
      "train loss:0.443090977697\n",
      "86\n",
      "train loss:0.555863142053\n",
      "87\n",
      "train loss:0.545052363624\n",
      "88\n",
      "train loss:0.717413351397\n",
      "89\n",
      "train loss:0.522952265124\n",
      "90\n",
      "train loss:0.68726238648\n",
      "91\n",
      "train loss:0.482293728364\n",
      "92\n",
      "train loss:0.543703893212\n",
      "93\n",
      "train loss:0.592903292555\n",
      "94\n",
      "train loss:0.483323679035\n",
      "95\n",
      "train loss:0.517994829137\n",
      "96\n",
      "train loss:0.465934337417\n",
      "97\n",
      "train loss:0.506790290939\n",
      "98\n",
      "train loss:0.580022363292\n",
      "99\n",
      "train loss:0.573008803665\n",
      "100\n",
      "train loss:0.514263390724\n",
      "101\n",
      "train loss:0.521834435308\n",
      "102\n",
      "train loss:0.50614294439\n",
      "103\n",
      "train loss:0.563604345277\n",
      "104\n",
      "train loss:0.536721380205\n",
      "105\n",
      "train loss:0.494292653046\n",
      "106\n",
      "train loss:0.472478817122\n",
      "107\n",
      "train loss:0.470958901854\n",
      "108\n",
      "train loss:0.474048297095\n",
      "109\n",
      "train loss:0.521895521681\n",
      "110\n",
      "train loss:0.605541659446\n",
      "111\n",
      "train loss:0.518812934757\n",
      "112\n",
      "train loss:0.571047771151\n",
      "113\n",
      "train loss:0.485416853561\n",
      "114\n",
      "train loss:0.583412965869\n",
      "115\n",
      "train loss:0.437985699979\n",
      "116\n",
      "train loss:0.454044855558\n",
      "117\n",
      "train loss:0.578782164482\n",
      "118\n",
      "train loss:0.626407781828\n",
      "119\n",
      "train loss:0.479467453636\n",
      "120\n",
      "train loss:0.527057311254\n",
      "121\n",
      "train loss:0.434589634868\n",
      "122\n",
      "train loss:0.510884733027\n",
      "123\n",
      "train loss:0.538015106438\n",
      "124\n",
      "train loss:0.499441419488\n",
      "125\n",
      "train loss:0.526806047972\n",
      "126\n",
      "train loss:0.638141769327\n",
      "127\n",
      "train loss:0.66580279366\n",
      "128\n",
      "train loss:0.418236821829\n",
      "129\n",
      "train loss:0.469746527985\n",
      "130\n",
      "train loss:0.580012995265\n",
      "131\n",
      "train loss:0.508733958338\n",
      "132\n",
      "train loss:0.515431240186\n",
      "133\n",
      "train loss:0.457978038469\n",
      "134\n",
      "train loss:0.498387204723\n",
      "135\n",
      "train loss:0.555090197019\n",
      "136\n",
      "train loss:0.660153892106\n",
      "137\n",
      "train loss:0.59125854639\n",
      "138\n",
      "train loss:0.517886768739\n",
      "139\n",
      "train loss:0.482977481704\n",
      "140\n",
      "train loss:0.602202905194\n",
      "141\n",
      "train loss:0.478172856333\n",
      "142\n",
      "train loss:0.424116508648\n",
      "143\n",
      "train loss:0.639717356458\n",
      "144\n",
      "train loss:0.494916879761\n",
      "145\n",
      "train loss:0.555510975604\n",
      "146\n",
      "train loss:0.463072925935\n",
      "147\n",
      "train loss:0.517896404224\n",
      "148\n",
      "train loss:0.466642069964\n",
      "149\n",
      "train loss:0.458338646003\n",
      "150\n",
      "train loss:0.465448863668\n",
      "151\n",
      "train loss:0.524152754407\n",
      "152\n",
      "train loss:0.587679906823\n",
      "153\n",
      "train loss:0.635142661651\n",
      "154\n",
      "train loss:0.450429817749\n",
      "155\n",
      "train loss:0.508496614585\n",
      "156\n",
      "train loss:0.506396921423\n",
      "157\n",
      "train loss:0.565585543623\n",
      "158\n",
      "train loss:0.498478392357\n",
      "159\n",
      "train loss:0.520840016158\n",
      "160\n",
      "train loss:0.509679436208\n",
      "161\n",
      "train loss:0.469410426014\n",
      "162\n",
      "train loss:0.566387170602\n",
      "163\n",
      "train loss:0.544006955576\n",
      "164\n",
      "train loss:0.559478900616\n",
      "165\n",
      "train loss:0.441312948981\n",
      "166\n",
      "train loss:0.450374975541\n",
      "167\n",
      "train loss:0.47995423393\n",
      "168\n",
      "train loss:0.410719047486\n",
      "169\n",
      "train loss:0.522552114587\n",
      "170\n",
      "train loss:0.46360648791\n",
      "171\n",
      "train loss:0.531210602792\n",
      "172\n",
      "train loss:0.559965852026\n",
      "173\n",
      "train loss:0.515467024645\n",
      "174\n",
      "train loss:0.546713655832\n",
      "175\n",
      "train loss:0.511861138616\n",
      "176\n",
      "train loss:0.685892371466\n",
      "177\n",
      "train loss:0.461543378738\n",
      "178\n",
      "train loss:0.482000717235\n",
      "179\n",
      "train loss:0.482223559298\n",
      "180\n",
      "train loss:0.464627415835\n",
      "181\n",
      "train loss:0.598421154463\n",
      "182\n",
      "train loss:0.475915230096\n",
      "183\n",
      "train loss:0.427077978172\n",
      "184\n",
      "train loss:0.504331244692\n",
      "185\n",
      "train loss:0.458368625671\n",
      "186\n",
      "train loss:0.459902178536\n",
      "187\n",
      "train loss:0.594177136595\n",
      "188\n",
      "train loss:0.503816947548\n",
      "189\n",
      "train loss:0.56861030485\n",
      "190\n",
      "train loss:0.51635749253\n",
      "191\n",
      "train loss:0.592815155808\n",
      "192\n",
      "train loss:0.444172144026\n",
      "193\n",
      "train loss:0.434223533557\n",
      "194\n",
      "train loss:0.465133997265\n",
      "195\n",
      "train loss:0.531506758948\n",
      "196\n",
      "train loss:0.510195997085\n",
      "197\n",
      "train loss:0.497561782379\n",
      "198\n",
      "train loss:0.558236784449\n",
      "199\n",
      "train loss:0.504211562139\n",
      "200\n",
      "train loss:0.434334576573\n",
      "201\n",
      "train loss:0.494682972356\n",
      "202\n",
      "train loss:0.524533362387\n",
      "203\n",
      "train loss:0.532104981064\n",
      "204\n",
      "train loss:0.527411423207\n",
      "205\n",
      "train loss:0.581119091301\n",
      "206\n",
      "train loss:0.478602004725\n",
      "207\n",
      "train loss:0.597961322574\n",
      "208\n",
      "train loss:0.577269400534\n",
      "209\n",
      "train loss:0.500660326132\n",
      "210\n",
      "train loss:0.629155600107\n",
      "211\n",
      "train loss:0.533378612944\n",
      "212\n",
      "train loss:0.564305047519\n",
      "213\n",
      "train loss:0.578089092209\n",
      "214\n",
      "train loss:0.616576445995\n",
      "215\n",
      "train loss:0.502466247092\n",
      "216\n",
      "train loss:0.535205332508\n",
      "217\n",
      "train loss:0.54441487953\n",
      "218\n",
      "train loss:0.438047486907\n",
      "219\n",
      "train loss:0.511130262722\n",
      "220\n",
      "train loss:0.4918504081\n",
      "221\n",
      "train loss:0.577232480772\n",
      "222\n",
      "train loss:0.452604222279\n",
      "223\n",
      "train loss:0.472880754553\n",
      "224\n",
      "train loss:0.542368184738\n",
      "225\n",
      "train loss:0.56128989342\n",
      "226\n",
      "train loss:0.515631183042\n",
      "227\n",
      "train loss:0.410447396933\n",
      "228\n",
      "train loss:0.444582510446\n",
      "229\n",
      "train loss:0.505711464448\n",
      "230\n",
      "train loss:0.665225537281\n",
      "231\n",
      "train loss:0.443192453903\n",
      "232\n",
      "train loss:0.502165852329\n",
      "233\n",
      "train loss:0.543489044195\n",
      "234\n",
      "train loss:0.452738138524\n",
      "235\n",
      "train loss:0.468644027939\n",
      "236\n",
      "train loss:0.441674171259\n",
      "237\n",
      "train loss:0.48103724418\n",
      "238\n",
      "train loss:0.557964016008\n",
      "239\n",
      "train loss:0.625001675977\n",
      "240\n",
      "train loss:0.598709107191\n",
      "241\n",
      "train loss:0.469683105238\n",
      "242\n",
      "train loss:0.551410003335\n",
      "243\n",
      "train loss:0.520129950291\n",
      "244\n",
      "train loss:0.432800157275\n",
      "245\n",
      "train loss:0.449394267767\n",
      "246\n",
      "train loss:0.507332136559\n",
      "247\n",
      "train loss:0.485367379816\n",
      "248\n",
      "train loss:0.524393184106\n",
      "249\n",
      "train loss:0.464587463073\n",
      "250\n",
      "train loss:0.480510360266\n",
      "251\n",
      "train loss:0.472937545\n",
      "252\n",
      "train loss:0.603615777348\n",
      "253\n",
      "train loss:0.466803915034\n",
      "254\n",
      "train loss:0.438811551172\n",
      "255\n",
      "train loss:0.469654102325\n",
      "256\n",
      "train loss:0.521469523886\n",
      "257\n",
      "train loss:0.537696920893\n",
      "258\n",
      "train loss:0.475675853565\n",
      "259\n",
      "train loss:0.577505820981\n",
      "260\n",
      "train loss:0.474015843335\n",
      "261\n",
      "train loss:0.420864947935\n",
      "262\n",
      "train loss:0.562680519321\n",
      "263\n",
      "train loss:0.518192621105\n",
      "264\n",
      "train loss:0.510664013625\n",
      "265\n",
      "train loss:0.492423295299\n",
      "266\n",
      "train loss:0.529683807522\n",
      "0\n",
      "train loss:0.51518569151\n",
      "7th epoch end: Saved Network Parameters!\n",
      "=== epoch:7, train acc:0.998, test acc:0.991 ===\n",
      "1\n",
      "train loss:0.605703316304\n",
      "2\n",
      "train loss:0.631669399392\n",
      "3\n",
      "train loss:0.540004064443\n",
      "4\n",
      "train loss:0.424018737332\n",
      "5\n",
      "train loss:0.491418789365\n",
      "6\n",
      "train loss:0.558151860229\n",
      "7\n",
      "train loss:0.518607152802\n",
      "8\n",
      "train loss:0.530194869208\n",
      "9\n",
      "train loss:0.468717605597\n",
      "10\n",
      "train loss:0.466707928555\n",
      "11\n",
      "train loss:0.610304595444\n",
      "12\n",
      "train loss:0.463842083975\n",
      "13\n",
      "train loss:0.517198760255\n",
      "14\n",
      "train loss:0.572629563296\n",
      "15\n",
      "train loss:0.500995083004\n",
      "16\n",
      "train loss:0.595809765994\n",
      "17\n",
      "train loss:0.569556479895\n",
      "18\n",
      "train loss:0.454541480114\n",
      "19\n",
      "train loss:0.53382113872\n",
      "20\n",
      "train loss:0.567672249523\n",
      "21\n",
      "train loss:0.501085170626\n",
      "22\n",
      "train loss:0.577404445938\n",
      "23\n",
      "train loss:0.42614559667\n",
      "24\n",
      "train loss:0.54058462598\n",
      "25\n",
      "train loss:0.507308477513\n",
      "26\n",
      "train loss:0.509162087472\n",
      "27\n",
      "train loss:0.636253219024\n",
      "28\n",
      "train loss:0.503028916269\n",
      "29\n",
      "train loss:0.476009485478\n",
      "30\n",
      "train loss:0.613101093964\n",
      "31\n",
      "train loss:0.464632980852\n",
      "32\n",
      "train loss:0.456440438085\n",
      "33\n",
      "train loss:0.536274748168\n",
      "34\n",
      "train loss:0.491797729439\n",
      "35\n",
      "train loss:0.500349041062\n",
      "36\n",
      "train loss:0.485442212569\n",
      "37\n",
      "train loss:0.586601009074\n",
      "38\n",
      "train loss:0.592071331763\n",
      "39\n",
      "train loss:0.495964657293\n",
      "40\n",
      "train loss:0.454917116644\n",
      "41\n",
      "train loss:0.519280105184\n",
      "42\n",
      "train loss:0.51905465415\n",
      "43\n",
      "train loss:0.494643168077\n",
      "44\n",
      "train loss:0.558401652555\n",
      "45\n",
      "train loss:0.530436243196\n",
      "46\n",
      "train loss:0.518399655526\n",
      "47\n",
      "train loss:0.530654956532\n",
      "48\n",
      "train loss:0.500871332807\n",
      "49\n",
      "train loss:0.531435841444\n",
      "50\n",
      "train loss:0.492508933879\n",
      "51\n",
      "train loss:0.566100163096\n",
      "52\n",
      "train loss:0.490106335438\n",
      "53\n",
      "train loss:0.456724374019\n",
      "54\n",
      "train loss:0.531495296357\n",
      "55\n",
      "train loss:0.471656604305\n",
      "56\n",
      "train loss:0.432836070216\n",
      "57\n",
      "train loss:0.478549467446\n",
      "58\n",
      "train loss:0.440614898627\n",
      "59\n",
      "train loss:0.486014335521\n",
      "60\n",
      "train loss:0.468010927561\n",
      "61\n",
      "train loss:0.531221198391\n",
      "62\n",
      "train loss:0.531749559984\n",
      "63\n",
      "train loss:0.475716136801\n",
      "64\n",
      "train loss:0.578531899271\n",
      "65\n",
      "train loss:0.446647883341\n",
      "66\n",
      "train loss:0.543511540081\n",
      "67\n",
      "train loss:0.631881542615\n",
      "68\n",
      "train loss:0.594452258902\n",
      "69\n",
      "train loss:0.4241604967\n",
      "70\n",
      "train loss:0.583161664644\n",
      "71\n",
      "train loss:0.575572557151\n",
      "72\n",
      "train loss:0.523846435612\n",
      "73\n",
      "train loss:0.482514299075\n",
      "74\n",
      "train loss:0.478175143023\n",
      "75\n",
      "train loss:0.451357916749\n",
      "76\n",
      "train loss:0.503335643926\n",
      "77\n",
      "train loss:0.545296946733\n",
      "78\n",
      "train loss:0.497174518228\n",
      "79\n",
      "train loss:0.594213848487\n",
      "80\n",
      "train loss:0.559417909869\n",
      "81\n",
      "train loss:0.496143543126\n",
      "82\n",
      "train loss:0.478300220344\n",
      "83\n",
      "train loss:0.504163677871\n",
      "84\n",
      "train loss:0.585132198919\n",
      "85\n",
      "train loss:0.486325697958\n",
      "86\n",
      "train loss:0.451432561159\n",
      "87\n",
      "train loss:0.57012937333\n",
      "88\n",
      "train loss:0.446418842674\n",
      "89\n",
      "train loss:0.552595170052\n",
      "90\n",
      "train loss:0.543157429839\n",
      "91\n",
      "train loss:0.49065723259\n",
      "92\n",
      "train loss:0.462831839689\n",
      "93\n",
      "train loss:0.549350728724\n",
      "94\n",
      "train loss:0.586270966517\n",
      "95\n",
      "train loss:0.512127883864\n",
      "96\n",
      "train loss:0.495239073043\n",
      "97\n",
      "train loss:0.573049867566\n",
      "98\n",
      "train loss:0.44682780091\n",
      "99\n",
      "train loss:0.527233139164\n",
      "100\n",
      "train loss:0.455653913058\n",
      "101\n",
      "train loss:0.647531404073\n",
      "102\n",
      "train loss:0.507239695306\n",
      "103\n",
      "train loss:0.51361142199\n",
      "104\n",
      "train loss:0.460461751589\n",
      "105\n",
      "train loss:0.468252807928\n",
      "106\n",
      "train loss:0.559765657887\n",
      "107\n",
      "train loss:0.47050440313\n",
      "108\n",
      "train loss:0.507138751093\n",
      "109\n",
      "train loss:0.490604616997\n",
      "110\n",
      "train loss:0.56587448123\n",
      "111\n",
      "train loss:0.556320300588\n",
      "112\n",
      "train loss:0.45890835123\n",
      "113\n",
      "train loss:0.451998989311\n",
      "114\n",
      "train loss:0.647215167079\n",
      "115\n",
      "train loss:0.511320358721\n",
      "116\n",
      "train loss:0.571075013551\n",
      "117\n",
      "train loss:0.552458958251\n",
      "118\n",
      "train loss:0.57872750222\n",
      "119\n",
      "train loss:0.471504138018\n",
      "120\n",
      "train loss:0.45605307974\n",
      "121\n",
      "train loss:0.360934682967\n",
      "122\n",
      "train loss:0.568677084984\n",
      "123\n",
      "train loss:0.499499071052\n",
      "124\n",
      "train loss:0.516048153181\n",
      "125\n",
      "train loss:0.474354223357\n",
      "126\n",
      "train loss:0.545063580694\n",
      "127\n",
      "train loss:0.451036613702\n",
      "128\n",
      "train loss:0.538814690886\n",
      "129\n",
      "train loss:0.65709344278\n",
      "130\n",
      "train loss:0.474664813849\n",
      "131\n",
      "train loss:0.549204477297\n",
      "132\n",
      "train loss:0.556090784731\n",
      "133\n",
      "train loss:0.421677133725\n",
      "134\n",
      "train loss:0.505688368899\n",
      "135\n",
      "train loss:0.518136334156\n",
      "136\n",
      "train loss:0.435318772796\n",
      "137\n",
      "train loss:0.453361738185\n",
      "138\n",
      "train loss:0.416839197323\n",
      "139\n",
      "train loss:0.429842751698\n",
      "140\n",
      "train loss:0.492289879207\n",
      "141\n",
      "train loss:0.536138859586\n",
      "142\n",
      "train loss:0.446325050256\n",
      "143\n",
      "train loss:0.448874585571\n",
      "144\n",
      "train loss:0.488448257773\n",
      "145\n",
      "train loss:0.536358009669\n",
      "146\n",
      "train loss:0.607138084397\n",
      "147\n",
      "train loss:0.537405381344\n",
      "148\n",
      "train loss:0.544295937229\n",
      "149\n",
      "train loss:0.462380146581\n",
      "150\n",
      "train loss:0.452959846083\n",
      "151\n",
      "train loss:0.497405100553\n",
      "152\n",
      "train loss:0.566938817653\n",
      "153\n",
      "train loss:0.518327085583\n",
      "154\n",
      "train loss:0.416378709437\n",
      "155\n",
      "train loss:0.437787827327\n",
      "156\n",
      "train loss:0.61851288214\n",
      "157\n",
      "train loss:0.534454719425\n",
      "158\n",
      "train loss:0.518189988097\n",
      "159\n",
      "train loss:0.497332146056\n",
      "160\n",
      "train loss:0.483153255965\n",
      "161\n",
      "train loss:0.489238827087\n",
      "162\n",
      "train loss:0.408743844624\n",
      "163\n",
      "train loss:0.442440145271\n",
      "164\n",
      "train loss:0.474603511055\n",
      "165\n",
      "train loss:0.54571220441\n",
      "166\n",
      "train loss:0.536497922812\n",
      "167\n",
      "train loss:0.500615050346\n",
      "168\n",
      "train loss:0.463729741028\n",
      "169\n",
      "train loss:0.527385490396\n",
      "170\n",
      "train loss:0.544168042292\n",
      "171\n",
      "train loss:0.498893208078\n",
      "172\n",
      "train loss:0.555163977267\n",
      "173\n",
      "train loss:0.432118761011\n",
      "174\n",
      "train loss:0.513960812673\n",
      "175\n",
      "train loss:0.532112438485\n",
      "176\n",
      "train loss:0.446078440301\n",
      "177\n",
      "train loss:0.571711459457\n",
      "178\n",
      "train loss:0.577941227233\n",
      "179\n",
      "train loss:0.512817663404\n",
      "180\n",
      "train loss:0.488690945427\n",
      "181\n",
      "train loss:0.442376640182\n",
      "182\n",
      "train loss:0.481956383418\n",
      "183\n",
      "train loss:0.494087305353\n",
      "184\n",
      "train loss:0.596369601757\n",
      "185\n",
      "train loss:0.495173017087\n",
      "186\n",
      "train loss:0.442023685438\n",
      "187\n",
      "train loss:0.6141776517\n",
      "188\n",
      "train loss:0.465405818006\n",
      "189\n",
      "train loss:0.491466435484\n",
      "190\n",
      "train loss:0.466568562942\n",
      "191\n",
      "train loss:0.522624355847\n",
      "192\n",
      "train loss:0.491355403891\n",
      "193\n",
      "train loss:0.558340790708\n",
      "194\n",
      "train loss:0.488010152376\n",
      "195\n",
      "train loss:0.486578343433\n",
      "196\n",
      "train loss:0.481445151974\n",
      "197\n",
      "train loss:0.503641598368\n",
      "198\n",
      "train loss:0.578959705416\n",
      "199\n",
      "train loss:0.586684049694\n",
      "200\n",
      "train loss:0.477033531521\n",
      "201\n",
      "train loss:0.57228402816\n",
      "202\n",
      "train loss:0.565114820307\n",
      "203\n",
      "train loss:0.441097650459\n",
      "204\n",
      "train loss:0.454336833496\n",
      "205\n",
      "train loss:0.469803809378\n",
      "206\n",
      "train loss:0.483883399321\n",
      "207\n",
      "train loss:0.528780342818\n",
      "208\n",
      "train loss:0.471110539114\n",
      "209\n",
      "train loss:0.526009258604\n",
      "210\n",
      "train loss:0.479028272522\n",
      "211\n",
      "train loss:0.419383219222\n",
      "212\n",
      "train loss:0.437690917349\n",
      "213\n",
      "train loss:0.446442402815\n",
      "214\n",
      "train loss:0.574077076139\n",
      "215\n",
      "train loss:0.552941993597\n",
      "216\n",
      "train loss:0.558401272805\n",
      "217\n",
      "train loss:0.458109662242\n",
      "218\n",
      "train loss:0.530709870311\n",
      "219\n",
      "train loss:0.452360225003\n",
      "220\n",
      "train loss:0.487824353954\n",
      "221\n",
      "train loss:0.579153871566\n",
      "222\n",
      "train loss:0.47676844594\n",
      "223\n",
      "train loss:0.443025305352\n",
      "224\n",
      "train loss:0.515620868625\n",
      "225\n",
      "train loss:0.506336195647\n",
      "226\n",
      "train loss:0.548373193193\n",
      "227\n",
      "train loss:0.483262882305\n",
      "228\n",
      "train loss:0.532751318992\n",
      "229\n",
      "train loss:0.449785495466\n",
      "230\n",
      "train loss:0.53750635583\n",
      "231\n",
      "train loss:0.473349203559\n",
      "232\n",
      "train loss:0.457834973313\n",
      "233\n",
      "train loss:0.523559937558\n",
      "234\n",
      "train loss:0.577506910463\n",
      "235\n",
      "train loss:0.483834951101\n",
      "236\n",
      "train loss:0.528300338231\n",
      "237\n",
      "train loss:0.416948715169\n",
      "238\n",
      "train loss:0.551321983573\n",
      "239\n",
      "train loss:0.651968171201\n",
      "240\n",
      "train loss:0.452126238959\n",
      "241\n",
      "train loss:0.591799370766\n",
      "242\n",
      "train loss:0.529866400731\n",
      "243\n",
      "train loss:0.403850707988\n",
      "244\n",
      "train loss:0.46961897548\n",
      "245\n",
      "train loss:0.434651716498\n",
      "246\n",
      "train loss:0.452661444847\n",
      "247\n",
      "train loss:0.528892908111\n",
      "248\n",
      "train loss:0.454454913329\n",
      "249\n",
      "train loss:0.510595384214\n",
      "250\n",
      "train loss:0.493156995753\n",
      "251\n",
      "train loss:0.574996902817\n",
      "252\n",
      "train loss:0.454089765762\n",
      "253\n",
      "train loss:0.602878733375\n",
      "254\n",
      "train loss:0.516709943852\n",
      "255\n",
      "train loss:0.531955560071\n",
      "256\n",
      "train loss:0.600723510054\n",
      "257\n",
      "train loss:0.472330054945\n",
      "258\n",
      "train loss:0.564902986805\n",
      "259\n",
      "train loss:0.550062346296\n",
      "260\n",
      "train loss:0.448018933098\n",
      "261\n",
      "train loss:0.498655342475\n",
      "262\n",
      "train loss:0.557627950734\n",
      "263\n",
      "train loss:0.393568911176\n",
      "264\n",
      "train loss:0.560135964272\n",
      "265\n",
      "train loss:0.575293375966\n",
      "266\n",
      "train loss:0.535106417455\n",
      "0\n",
      "train loss:0.699663506442\n",
      "8th epoch end: Saved Network Parameters!\n",
      "=== epoch:8, train acc:0.999, test acc:0.994 ===\n",
      "1\n",
      "train loss:0.541054932168\n",
      "2\n",
      "train loss:0.727409753667\n",
      "3\n",
      "train loss:0.474550450159\n",
      "4\n",
      "train loss:0.525640972449\n",
      "5\n",
      "train loss:0.582078525967\n",
      "6\n",
      "train loss:0.550251863593\n",
      "7\n",
      "train loss:0.52760685486\n",
      "8\n",
      "train loss:0.458769892882\n",
      "9\n",
      "train loss:0.502438630289\n",
      "10\n",
      "train loss:0.505342605804\n",
      "11\n",
      "train loss:0.47810265573\n",
      "12\n",
      "train loss:0.510701124924\n",
      "13\n",
      "train loss:0.501742220279\n",
      "14\n",
      "train loss:0.52913131808\n",
      "15\n",
      "train loss:0.469872399331\n",
      "16\n",
      "train loss:0.567267607628\n",
      "17\n",
      "train loss:0.551214566228\n",
      "18\n",
      "train loss:0.40746889332\n",
      "19\n",
      "train loss:0.46453758407\n",
      "20\n",
      "train loss:0.528747336736\n",
      "21\n",
      "train loss:0.463250773195\n",
      "22\n",
      "train loss:0.496474911234\n",
      "23\n",
      "train loss:0.438579995776\n",
      "24\n",
      "train loss:0.600313076659\n",
      "25\n",
      "train loss:0.523139358846\n",
      "26\n",
      "train loss:0.481916101954\n",
      "27\n",
      "train loss:0.473215680438\n",
      "28\n",
      "train loss:0.589384772602\n",
      "29\n",
      "train loss:0.549303443506\n",
      "30\n",
      "train loss:0.43291489564\n",
      "31\n",
      "train loss:0.566076174324\n",
      "32\n",
      "train loss:0.526133797802\n",
      "33\n",
      "train loss:0.41535417204\n",
      "34\n",
      "train loss:0.581925770579\n",
      "35\n",
      "train loss:0.365359695747\n",
      "36\n",
      "train loss:0.517536284103\n",
      "37\n",
      "train loss:0.479360645577\n",
      "38\n",
      "train loss:0.507899398576\n",
      "39\n",
      "train loss:0.542972991453\n",
      "40\n",
      "train loss:0.613715943292\n",
      "41\n",
      "train loss:0.474478709717\n",
      "42\n",
      "train loss:0.579023168163\n",
      "43\n",
      "train loss:0.394059762502\n",
      "44\n",
      "train loss:0.460631543819\n",
      "45\n",
      "train loss:0.613726423976\n",
      "46\n",
      "train loss:0.553575002273\n",
      "47\n",
      "train loss:0.498647392322\n",
      "48\n",
      "train loss:0.648043450141\n",
      "49\n",
      "train loss:0.568041525812\n",
      "50\n",
      "train loss:0.415296867106\n",
      "51\n",
      "train loss:0.415675689637\n",
      "52\n",
      "train loss:0.593140421494\n",
      "53\n",
      "train loss:0.560273345527\n",
      "54\n",
      "train loss:0.544282794145\n",
      "55\n",
      "train loss:0.525737810791\n",
      "56\n",
      "train loss:0.51472972161\n",
      "57\n",
      "train loss:0.511262925832\n",
      "58\n",
      "train loss:0.559063587631\n",
      "59\n",
      "train loss:0.523761351591\n",
      "60\n",
      "train loss:0.479547685031\n",
      "61\n",
      "train loss:0.511468526311\n",
      "62\n",
      "train loss:0.485161528403\n",
      "63\n",
      "train loss:0.47347052665\n",
      "64\n",
      "train loss:0.509703391431\n",
      "65\n",
      "train loss:0.515301328604\n",
      "66\n",
      "train loss:0.534197882679\n",
      "67\n",
      "train loss:0.424848281903\n",
      "68\n",
      "train loss:0.547097570304\n",
      "69\n",
      "train loss:0.468545519715\n",
      "70\n",
      "train loss:0.460488461453\n",
      "71\n",
      "train loss:0.562476499527\n",
      "72\n",
      "train loss:0.478169807984\n",
      "73\n",
      "train loss:0.525580046821\n",
      "74\n",
      "train loss:0.495513004105\n",
      "75\n",
      "train loss:0.59125686305\n",
      "76\n",
      "train loss:0.492727617402\n",
      "77\n",
      "train loss:0.449620270073\n",
      "78\n",
      "train loss:0.578272009045\n",
      "79\n",
      "train loss:0.45441880989\n",
      "80\n",
      "train loss:0.573170348012\n",
      "81\n",
      "train loss:0.553272976435\n",
      "82\n",
      "train loss:0.432747730956\n",
      "83\n",
      "train loss:0.593041968816\n",
      "84\n",
      "train loss:0.495156245143\n",
      "85\n",
      "train loss:0.454488918083\n",
      "86\n",
      "train loss:0.47033909302\n",
      "87\n",
      "train loss:0.440597620149\n",
      "88\n",
      "train loss:0.598675011756\n",
      "89\n",
      "train loss:0.447503280676\n",
      "90\n",
      "train loss:0.502438613949\n",
      "91\n",
      "train loss:0.447291270325\n",
      "92\n",
      "train loss:0.475598043605\n",
      "93\n",
      "train loss:0.44178457167\n",
      "94\n",
      "train loss:0.549625869783\n",
      "95\n",
      "train loss:0.499905114173\n",
      "96\n",
      "train loss:0.649646380301\n",
      "97\n",
      "train loss:0.496021794135\n",
      "98\n",
      "train loss:0.589387333237\n",
      "99\n",
      "train loss:0.473989200332\n",
      "100\n",
      "train loss:0.660735210962\n",
      "101\n",
      "train loss:0.489950398846\n",
      "102\n",
      "train loss:0.449902702806\n",
      "103\n",
      "train loss:0.458475243152\n",
      "104\n",
      "train loss:0.595693264527\n",
      "105\n",
      "train loss:0.416908415111\n",
      "106\n",
      "train loss:0.506434649227\n",
      "107\n",
      "train loss:0.465950447865\n",
      "108\n",
      "train loss:0.508879634065\n",
      "109\n",
      "train loss:0.653729679671\n",
      "110\n",
      "train loss:0.65701964087\n",
      "111\n",
      "train loss:0.51589316807\n",
      "112\n",
      "train loss:0.496143805524\n",
      "113\n",
      "train loss:0.506881784708\n",
      "114\n",
      "train loss:0.445616554833\n",
      "115\n",
      "train loss:0.4798022411\n",
      "116\n",
      "train loss:0.499269436504\n",
      "117\n",
      "train loss:0.565494887463\n",
      "118\n",
      "train loss:0.444586204051\n",
      "119\n",
      "train loss:0.454618208532\n",
      "120\n",
      "train loss:0.491197935811\n",
      "121\n",
      "train loss:0.449313389629\n",
      "122\n",
      "train loss:0.551106275941\n",
      "123\n",
      "train loss:0.449382069101\n",
      "124\n",
      "train loss:0.510712579048\n",
      "125\n",
      "train loss:0.487553613464\n",
      "126\n",
      "train loss:0.460608736078\n",
      "127\n",
      "train loss:0.605149443907\n",
      "128\n",
      "train loss:0.45802492056\n",
      "129\n",
      "train loss:0.392257781848\n",
      "130\n",
      "train loss:0.519311171557\n",
      "131\n",
      "train loss:0.451543343518\n",
      "132\n",
      "train loss:0.455823891593\n",
      "133\n",
      "train loss:0.550729317421\n",
      "134\n",
      "train loss:0.458232871773\n",
      "135\n",
      "train loss:0.461801096803\n",
      "136\n",
      "train loss:0.588228844504\n",
      "137\n",
      "train loss:0.586800911421\n",
      "138\n",
      "train loss:0.475936315754\n",
      "139\n",
      "train loss:0.458331278956\n",
      "140\n",
      "train loss:0.45805426251\n",
      "141\n",
      "train loss:0.49314904236\n",
      "142\n",
      "train loss:0.534220927765\n",
      "143\n",
      "train loss:0.503527620636\n",
      "144\n",
      "train loss:0.51332297942\n",
      "145\n",
      "train loss:0.513001645177\n",
      "146\n",
      "train loss:0.434793896406\n",
      "147\n",
      "train loss:0.643262310722\n",
      "148\n",
      "train loss:0.545530721441\n",
      "149\n",
      "train loss:0.417707696082\n",
      "150\n",
      "train loss:0.414401987641\n",
      "151\n",
      "train loss:0.539090760343\n",
      "152\n",
      "train loss:0.622045907308\n",
      "153\n",
      "train loss:0.448360533454\n",
      "154\n",
      "train loss:0.503572429841\n",
      "155\n",
      "train loss:0.569473520765\n",
      "156\n",
      "train loss:0.438934292334\n",
      "157\n",
      "train loss:0.586365941044\n",
      "158\n",
      "train loss:0.550806986616\n",
      "159\n",
      "train loss:0.53231128132\n",
      "160\n",
      "train loss:0.459214752153\n",
      "161\n",
      "train loss:0.551188628138\n",
      "162\n",
      "train loss:0.6209344033\n",
      "163\n",
      "train loss:0.558517671422\n",
      "164\n",
      "train loss:0.470164890493\n",
      "165\n",
      "train loss:0.487569057163\n",
      "166\n",
      "train loss:0.463244789548\n",
      "167\n",
      "train loss:0.415555327626\n",
      "168\n",
      "train loss:0.45809568421\n",
      "169\n",
      "train loss:0.495754551726\n",
      "170\n",
      "train loss:0.471518918134\n",
      "171\n",
      "train loss:0.477737793998\n",
      "172\n",
      "train loss:0.461807271323\n",
      "173\n",
      "train loss:0.496583087008\n",
      "174\n",
      "train loss:0.554999916121\n",
      "175\n",
      "train loss:0.500340650344\n",
      "176\n",
      "train loss:0.616454639594\n",
      "177\n",
      "train loss:0.398563437245\n",
      "178\n",
      "train loss:0.439721309007\n",
      "179\n",
      "train loss:0.404228332204\n",
      "180\n",
      "train loss:0.429327965195\n",
      "181\n",
      "train loss:0.546996402865\n",
      "182\n",
      "train loss:0.498305070946\n",
      "183\n",
      "train loss:0.350228912079\n",
      "184\n",
      "train loss:0.583203939894\n",
      "185\n",
      "train loss:0.441246558646\n",
      "186\n",
      "train loss:0.56537278318\n",
      "187\n",
      "train loss:0.475388927604\n",
      "188\n",
      "train loss:0.520263252802\n",
      "189\n",
      "train loss:0.520596770368\n",
      "190\n",
      "train loss:0.435779999526\n",
      "191\n",
      "train loss:0.604033505192\n",
      "192\n",
      "train loss:0.466422616311\n",
      "193\n",
      "train loss:0.597217336442\n",
      "194\n",
      "train loss:0.509382743243\n",
      "195\n",
      "train loss:0.461135338896\n",
      "196\n",
      "train loss:0.430655835307\n",
      "197\n",
      "train loss:0.505162902646\n",
      "198\n",
      "train loss:0.466743712556\n",
      "199\n",
      "train loss:0.458488393139\n",
      "200\n",
      "train loss:0.467044052138\n",
      "201\n",
      "train loss:0.490608133064\n",
      "202\n",
      "train loss:0.523666021506\n",
      "203\n",
      "train loss:0.443277269422\n",
      "204\n",
      "train loss:0.537555496674\n",
      "205\n",
      "train loss:0.521476339193\n",
      "206\n",
      "train loss:0.454928169507\n",
      "207\n",
      "train loss:0.439438435211\n",
      "208\n",
      "train loss:0.510132693158\n",
      "209\n",
      "train loss:0.454826546952\n",
      "210\n",
      "train loss:0.464427610816\n",
      "211\n",
      "train loss:0.447510006727\n",
      "212\n",
      "train loss:0.408035827766\n",
      "213\n",
      "train loss:0.52050466971\n",
      "214\n",
      "train loss:0.536687964494\n",
      "215\n",
      "train loss:0.507288357733\n",
      "216\n",
      "train loss:0.432155542187\n",
      "217\n",
      "train loss:0.409081869897\n",
      "218\n",
      "train loss:0.526895527209\n",
      "219\n",
      "train loss:0.630389944893\n",
      "220\n",
      "train loss:0.53949821237\n",
      "221\n",
      "train loss:0.483762602893\n",
      "222\n",
      "train loss:0.533566326927\n",
      "223\n",
      "train loss:0.591897335503\n",
      "224\n",
      "train loss:0.445988167128\n",
      "225\n",
      "train loss:0.512669883987\n",
      "226\n",
      "train loss:0.62564777293\n",
      "227\n",
      "train loss:0.552709311779\n",
      "228\n",
      "train loss:0.478865335069\n",
      "229\n",
      "train loss:0.5889288939\n",
      "230\n",
      "train loss:0.483516677049\n",
      "231\n",
      "train loss:0.50254722945\n",
      "232\n",
      "train loss:0.405748218098\n",
      "233\n",
      "train loss:0.59576219035\n",
      "234\n",
      "train loss:0.390468372619\n",
      "235\n",
      "train loss:0.523336426804\n",
      "236\n",
      "train loss:0.552902897453\n",
      "237\n",
      "train loss:0.450586770281\n",
      "238\n",
      "train loss:0.498888694366\n",
      "239\n",
      "train loss:0.580945146735\n",
      "240\n",
      "train loss:0.498998371137\n",
      "241\n",
      "train loss:0.551739039297\n",
      "242\n",
      "train loss:0.429342183554\n",
      "243\n",
      "train loss:0.437917436492\n",
      "244\n",
      "train loss:0.539983442265\n",
      "245\n",
      "train loss:0.467026075507\n",
      "246\n",
      "train loss:0.442979793915\n",
      "247\n",
      "train loss:0.414518384971\n",
      "248\n",
      "train loss:0.449173813589\n",
      "249\n",
      "train loss:0.454667532519\n",
      "250\n",
      "train loss:0.412897952179\n",
      "251\n",
      "train loss:0.441477693142\n",
      "252\n",
      "train loss:0.437679774302\n",
      "253\n",
      "train loss:0.533274206323\n",
      "254\n",
      "train loss:0.51481635507\n",
      "255\n",
      "train loss:0.488724268771\n",
      "256\n",
      "train loss:0.516307019695\n",
      "257\n",
      "train loss:0.378360337661\n",
      "258\n",
      "train loss:0.456693031904\n",
      "259\n",
      "train loss:0.502385475482\n",
      "260\n",
      "train loss:0.415799154056\n",
      "261\n",
      "train loss:0.40841785949\n",
      "262\n",
      "train loss:0.429631013229\n",
      "263\n",
      "train loss:0.41786694039\n",
      "264\n",
      "train loss:0.60283168852\n",
      "265\n",
      "train loss:0.451504962439\n",
      "266\n",
      "train loss:0.512801493573\n",
      "0\n",
      "train loss:0.437415682114\n",
      "9th epoch end: Saved Network Parameters!\n",
      "=== epoch:9, train acc:0.999, test acc:0.993 ===\n",
      "1\n",
      "train loss:0.490644925646\n",
      "2\n",
      "train loss:0.550406890955\n",
      "3\n",
      "train loss:0.424460235213\n",
      "4\n",
      "train loss:0.586348422048\n",
      "5\n",
      "train loss:0.513691240028\n",
      "6\n",
      "train loss:0.467904577109\n",
      "7\n",
      "train loss:0.474594243092\n",
      "8\n",
      "train loss:0.701660735093\n",
      "9\n",
      "train loss:0.377706861561\n",
      "10\n",
      "train loss:0.431442435927\n",
      "11\n",
      "train loss:0.478938569085\n",
      "12\n",
      "train loss:0.475116007067\n",
      "13\n",
      "train loss:0.53417115017\n",
      "14\n",
      "train loss:0.4499241443\n",
      "15\n",
      "train loss:0.439779446694\n",
      "16\n",
      "train loss:0.568532054036\n",
      "17\n",
      "train loss:0.539673271399\n",
      "18\n",
      "train loss:0.611602709148\n",
      "19\n",
      "train loss:0.507925521845\n",
      "20\n",
      "train loss:0.54857971026\n",
      "21\n",
      "train loss:0.445057459487\n",
      "22\n",
      "train loss:0.481949402302\n",
      "23\n",
      "train loss:0.46733413377\n",
      "24\n",
      "train loss:0.67551758839\n",
      "25\n",
      "train loss:0.471120919647\n",
      "26\n",
      "train loss:0.45305155883\n",
      "27\n",
      "train loss:0.49613081804\n",
      "28\n",
      "train loss:0.435885320912\n",
      "29\n",
      "train loss:0.47932078715\n",
      "30\n",
      "train loss:0.602706509325\n",
      "31\n",
      "train loss:0.539662350043\n",
      "32\n",
      "train loss:0.385817011228\n",
      "33\n",
      "train loss:0.430815345415\n",
      "34\n",
      "train loss:0.495471577839\n",
      "35\n",
      "train loss:0.502159545505\n",
      "36\n",
      "train loss:0.527211253798\n",
      "37\n",
      "train loss:0.457166512484\n",
      "38\n",
      "train loss:0.433795836092\n",
      "39\n",
      "train loss:0.619793651044\n",
      "40\n",
      "train loss:0.454552063836\n",
      "41\n",
      "train loss:0.515093202722\n",
      "42\n",
      "train loss:0.40579998675\n",
      "43\n",
      "train loss:0.448681371275\n",
      "44\n",
      "train loss:0.457079480339\n",
      "45\n",
      "train loss:0.416893759012\n",
      "46\n",
      "train loss:0.441102365524\n",
      "47\n",
      "train loss:0.523071853401\n",
      "48\n",
      "train loss:0.41923357138\n",
      "49\n",
      "train loss:0.481220800987\n",
      "50\n",
      "train loss:0.44428324025\n",
      "51\n",
      "train loss:0.47243869229\n",
      "52\n",
      "train loss:0.522395513834\n",
      "53\n",
      "train loss:0.453251581118\n",
      "54\n",
      "train loss:0.422182616165\n",
      "55\n",
      "train loss:0.474500703768\n",
      "56\n",
      "train loss:0.475044266281\n",
      "57\n",
      "train loss:0.507866963415\n",
      "58\n",
      "train loss:0.464760643365\n",
      "59\n",
      "train loss:0.521859289683\n",
      "60\n",
      "train loss:0.56409801032\n",
      "61\n",
      "train loss:0.430785310192\n",
      "62\n",
      "train loss:0.437451376665\n",
      "63\n",
      "train loss:0.556942324918\n",
      "64\n",
      "train loss:0.462842495488\n",
      "65\n",
      "train loss:0.493210100589\n",
      "66\n",
      "train loss:0.528194428934\n",
      "67\n",
      "train loss:0.471254936088\n",
      "68\n",
      "train loss:0.524462248934\n",
      "69\n",
      "train loss:0.360497405279\n",
      "70\n",
      "train loss:0.553438050315\n",
      "71\n",
      "train loss:0.455859943065\n",
      "72\n",
      "train loss:0.410900835532\n",
      "73\n",
      "train loss:0.554073890811\n",
      "74\n",
      "train loss:0.517223047136\n",
      "75\n",
      "train loss:0.462118917739\n",
      "76\n",
      "train loss:0.467599796618\n",
      "77\n",
      "train loss:0.511246990459\n",
      "78\n",
      "train loss:0.445599282123\n",
      "79\n",
      "train loss:0.463886739749\n",
      "80\n",
      "train loss:0.489652320929\n",
      "81\n",
      "train loss:0.509946000889\n",
      "82\n",
      "train loss:0.526684881329\n",
      "83\n",
      "train loss:0.541465438836\n",
      "84\n",
      "train loss:0.581255902353\n",
      "85\n",
      "train loss:0.428169641965\n",
      "86\n",
      "train loss:0.40570973168\n",
      "87\n",
      "train loss:0.453934324484\n",
      "88\n",
      "train loss:0.562000476136\n",
      "89\n",
      "train loss:0.52026780748\n",
      "90\n",
      "train loss:0.416022708202\n",
      "91\n",
      "train loss:0.582189756783\n",
      "92\n",
      "train loss:0.491078309279\n",
      "93\n",
      "train loss:0.427468965932\n",
      "94\n",
      "train loss:0.537470201962\n",
      "95\n",
      "train loss:0.442444209344\n",
      "96\n",
      "train loss:0.479493155066\n",
      "97\n",
      "train loss:0.475969852924\n",
      "98\n",
      "train loss:0.440202852595\n",
      "99\n",
      "train loss:0.538568345481\n",
      "100\n",
      "train loss:0.483150066752\n",
      "101\n",
      "train loss:0.542372431136\n",
      "102\n",
      "train loss:0.539120972494\n",
      "103\n",
      "train loss:0.56453917529\n",
      "104\n",
      "train loss:0.585542401735\n",
      "105\n",
      "train loss:0.63660319589\n",
      "106\n",
      "train loss:0.435375551498\n",
      "107\n",
      "train loss:0.538749092666\n",
      "108\n",
      "train loss:0.462103802714\n",
      "109\n",
      "train loss:0.487411389674\n",
      "110\n",
      "train loss:0.530947402002\n",
      "111\n",
      "train loss:0.402824755523\n",
      "112\n",
      "train loss:0.65281959923\n",
      "113\n",
      "train loss:0.539213205664\n",
      "114\n",
      "train loss:0.427503084005\n",
      "115\n",
      "train loss:0.591980029142\n",
      "116\n",
      "train loss:0.47787197572\n",
      "117\n",
      "train loss:0.438994894395\n",
      "118\n",
      "train loss:0.49174358977\n",
      "119\n",
      "train loss:0.363928860051\n",
      "120\n",
      "train loss:0.535622955318\n",
      "121\n",
      "train loss:0.465868203713\n",
      "122\n",
      "train loss:0.599845562934\n",
      "123\n",
      "train loss:0.48008722546\n",
      "124\n",
      "train loss:0.491557954854\n",
      "125\n",
      "train loss:0.472931059347\n",
      "126\n",
      "train loss:0.535385075428\n",
      "127\n",
      "train loss:0.502251236567\n",
      "128\n",
      "train loss:0.486859341436\n",
      "129\n",
      "train loss:0.479508169587\n",
      "130\n",
      "train loss:0.527273101775\n",
      "131\n",
      "train loss:0.568044319631\n",
      "132\n",
      "train loss:0.490611358694\n",
      "133\n",
      "train loss:0.477253488391\n",
      "134\n",
      "train loss:0.601424793991\n",
      "135\n",
      "train loss:0.505754927886\n",
      "136\n",
      "train loss:0.407218359461\n",
      "137\n",
      "train loss:0.539511847692\n",
      "138\n",
      "train loss:0.50583516715\n",
      "139\n",
      "train loss:0.529372020886\n",
      "140\n",
      "train loss:0.567745344073\n",
      "141\n",
      "train loss:0.44993515344\n",
      "142\n",
      "train loss:0.477910351044\n",
      "143\n",
      "train loss:0.515981143633\n",
      "144\n",
      "train loss:0.414510500403\n",
      "145\n",
      "train loss:0.488890546355\n",
      "146\n",
      "train loss:0.449127277854\n",
      "147\n",
      "train loss:0.482383716884\n",
      "148\n",
      "train loss:0.471911947836\n",
      "149\n",
      "train loss:0.481736992908\n",
      "150\n",
      "train loss:0.46565865558\n",
      "151\n",
      "train loss:0.54161165995\n",
      "152\n",
      "train loss:0.429122939134\n",
      "153\n",
      "train loss:0.4715226743\n",
      "154\n",
      "train loss:0.449499777703\n",
      "155\n",
      "train loss:0.448821538593\n",
      "156\n",
      "train loss:0.436315307688\n",
      "157\n",
      "train loss:0.428471982984\n",
      "158\n",
      "train loss:0.439636401242\n",
      "159\n",
      "train loss:0.421555030862\n",
      "160\n",
      "train loss:0.441599652263\n",
      "161\n",
      "train loss:0.412964845109\n",
      "162\n",
      "train loss:0.454836377567\n",
      "163\n",
      "train loss:0.546070008477\n",
      "164\n",
      "train loss:0.449164644222\n",
      "165\n",
      "train loss:0.512805771579\n",
      "166\n",
      "train loss:0.643945696291\n",
      "167\n",
      "train loss:0.521048537054\n",
      "168\n",
      "train loss:0.500340261474\n",
      "169\n",
      "train loss:0.492213810159\n",
      "170\n",
      "train loss:0.530980189423\n",
      "171\n",
      "train loss:0.475284034653\n",
      "172\n",
      "train loss:0.512598482411\n",
      "173\n",
      "train loss:0.438915169848\n",
      "174\n",
      "train loss:0.556965230726\n",
      "175\n",
      "train loss:0.522859752266\n",
      "176\n",
      "train loss:0.405792828685\n",
      "177\n",
      "train loss:0.535339743792\n",
      "178\n",
      "train loss:0.564593016741\n",
      "179\n",
      "train loss:0.491579356784\n",
      "180\n",
      "train loss:0.522595097008\n",
      "181\n",
      "train loss:0.525349761288\n",
      "182\n",
      "train loss:0.582082565537\n",
      "183\n",
      "train loss:0.526552282442\n",
      "184\n",
      "train loss:0.508720052332\n",
      "185\n",
      "train loss:0.389321693545\n",
      "186\n",
      "train loss:0.485789986609\n",
      "187\n",
      "train loss:0.417322297038\n",
      "188\n",
      "train loss:0.422850793501\n",
      "189\n",
      "train loss:0.433648147999\n",
      "190\n",
      "train loss:0.473616822843\n",
      "191\n",
      "train loss:0.473363626365\n",
      "192\n",
      "train loss:0.67253360246\n",
      "193\n",
      "train loss:0.542126806608\n",
      "194\n",
      "train loss:0.467085816617\n",
      "195\n",
      "train loss:0.37642243414\n",
      "196\n",
      "train loss:0.440681119217\n",
      "197\n",
      "train loss:0.451680031474\n",
      "198\n",
      "train loss:0.4118527565\n",
      "199\n",
      "train loss:0.396432512905\n",
      "200\n",
      "train loss:0.507008539722\n",
      "201\n",
      "train loss:0.428866128369\n",
      "202\n",
      "train loss:0.426369411385\n",
      "203\n",
      "train loss:0.338102594419\n",
      "204\n",
      "train loss:0.513419150009\n",
      "205\n",
      "train loss:0.49749329324\n",
      "206\n",
      "train loss:0.511772855154\n",
      "207\n",
      "train loss:0.440571087925\n",
      "208\n",
      "train loss:0.406824247375\n",
      "209\n",
      "train loss:0.465286492888\n",
      "210\n",
      "train loss:0.481313592531\n",
      "211\n",
      "train loss:0.436625825245\n",
      "212\n",
      "train loss:0.523771524587\n",
      "213\n",
      "train loss:0.473021812044\n",
      "214\n",
      "train loss:0.434505262056\n",
      "215\n",
      "train loss:0.47354670891\n",
      "216\n",
      "train loss:0.65390685436\n",
      "217\n",
      "train loss:0.475207671193\n",
      "218\n",
      "train loss:0.468273899657\n",
      "219\n",
      "train loss:0.450961120838\n",
      "220\n",
      "train loss:0.457112352179\n",
      "221\n",
      "train loss:0.373588800106\n",
      "222\n",
      "train loss:0.506609635949\n",
      "223\n",
      "train loss:0.434526013523\n",
      "224\n",
      "train loss:0.518804647211\n",
      "225\n",
      "train loss:0.446649638888\n",
      "226\n",
      "train loss:0.483329215256\n",
      "227\n",
      "train loss:0.574109164553\n",
      "228\n",
      "train loss:0.617107722167\n",
      "229\n",
      "train loss:0.385397886181\n",
      "230\n",
      "train loss:0.452490790424\n",
      "231\n",
      "train loss:0.49396036544\n",
      "232\n",
      "train loss:0.501798440894\n",
      "233\n",
      "train loss:0.379214755263\n",
      "234\n",
      "train loss:0.468900240755\n",
      "235\n",
      "train loss:0.429951866722\n",
      "236\n",
      "train loss:0.504074012514\n",
      "237\n",
      "train loss:0.43232877095\n",
      "238\n",
      "train loss:0.453142820231\n",
      "239\n",
      "train loss:0.509430367148\n",
      "240\n",
      "train loss:0.544435447208\n",
      "241\n",
      "train loss:0.440854534992\n",
      "242\n",
      "train loss:0.497979108863\n",
      "243\n",
      "train loss:0.485650093978\n",
      "244\n",
      "train loss:0.382336704713\n",
      "245\n",
      "train loss:0.396950327988\n",
      "246\n",
      "train loss:0.46580273376\n",
      "247\n",
      "train loss:0.437904158733\n",
      "248\n",
      "train loss:0.515488658303\n",
      "249\n",
      "train loss:0.541144719976\n",
      "250\n",
      "train loss:0.421199068734\n",
      "251\n",
      "train loss:0.506709751312\n",
      "252\n",
      "train loss:0.393900728938\n",
      "253\n",
      "train loss:0.490939526435\n",
      "254\n",
      "train loss:0.465171340009\n",
      "255\n",
      "train loss:0.504624258312\n",
      "256\n",
      "train loss:0.505375545316\n",
      "257\n",
      "train loss:0.407788017016\n",
      "258\n",
      "train loss:0.552230270828\n",
      "259\n",
      "train loss:0.507942287744\n",
      "260\n",
      "train loss:0.550534024908\n",
      "261\n",
      "train loss:0.355695111288\n",
      "262\n",
      "train loss:0.550648905565\n",
      "263\n",
      "train loss:0.480269481603\n",
      "264\n",
      "train loss:0.495873733189\n",
      "265\n",
      "train loss:0.437547824625\n",
      "266\n",
      "train loss:0.578733501342\n",
      "0\n",
      "train loss:0.54635803937\n",
      "10th epoch end: Saved Network Parameters!\n",
      "=== epoch:10, train acc:0.999, test acc:0.995 ===\n",
      "1\n",
      "train loss:0.47286654544\n",
      "2\n",
      "train loss:0.52413728095\n",
      "3\n",
      "train loss:0.478325282793\n",
      "4\n",
      "train loss:0.471268150546\n",
      "5\n",
      "train loss:0.52429199063\n",
      "6\n",
      "train loss:0.542917872526\n",
      "7\n",
      "train loss:0.467988324446\n",
      "8\n",
      "train loss:0.402217879083\n",
      "9\n",
      "train loss:0.412945245423\n",
      "10\n",
      "train loss:0.372270510473\n",
      "11\n",
      "train loss:0.507714185499\n",
      "12\n",
      "train loss:0.429711580855\n",
      "13\n",
      "train loss:0.496337303697\n",
      "14\n",
      "train loss:0.58519777189\n",
      "15\n",
      "train loss:0.575833283178\n",
      "16\n",
      "train loss:0.471638068334\n",
      "17\n",
      "train loss:0.392395660355\n",
      "18\n",
      "train loss:0.613608909375\n",
      "19\n",
      "train loss:0.510588620329\n",
      "20\n",
      "train loss:0.47916723864\n",
      "21\n",
      "train loss:0.442297348477\n",
      "22\n",
      "train loss:0.585996346615\n",
      "23\n",
      "train loss:0.403979945235\n",
      "24\n",
      "train loss:0.515747051738\n",
      "25\n",
      "train loss:0.490849660707\n",
      "26\n",
      "train loss:0.509457071237\n",
      "27\n",
      "train loss:0.457904818966\n",
      "28\n",
      "train loss:0.376051878567\n",
      "29\n",
      "train loss:0.4768370963\n",
      "30\n",
      "train loss:0.42467157913\n",
      "31\n",
      "train loss:0.587692769092\n",
      "32\n",
      "train loss:0.404795641966\n",
      "33\n",
      "train loss:0.533218753257\n",
      "34\n",
      "train loss:0.535710333361\n",
      "35\n",
      "train loss:0.392025204265\n",
      "36\n",
      "train loss:0.698306828689\n",
      "37\n",
      "train loss:0.527649709662\n",
      "38\n",
      "train loss:0.452777664215\n",
      "39\n",
      "train loss:0.492945195011\n",
      "40\n",
      "train loss:0.446227639812\n",
      "41\n",
      "train loss:0.512324174248\n",
      "42\n",
      "train loss:0.429820051209\n",
      "43\n",
      "train loss:0.520945532945\n",
      "44\n",
      "train loss:0.437671717502\n",
      "45\n",
      "train loss:0.400636272033\n",
      "46\n",
      "train loss:0.544593725566\n",
      "47\n",
      "train loss:0.418143295566\n",
      "48\n",
      "train loss:0.492386602667\n",
      "49\n",
      "train loss:0.446125465888\n",
      "50\n",
      "train loss:0.426598327744\n",
      "51\n",
      "train loss:0.465300684656\n",
      "52\n",
      "train loss:0.450534937553\n",
      "53\n",
      "train loss:0.52461530873\n",
      "54\n",
      "train loss:0.460284781946\n",
      "55\n",
      "train loss:0.68549701456\n",
      "56\n",
      "train loss:0.532440259683\n",
      "57\n",
      "train loss:0.469528046517\n",
      "58\n",
      "train loss:0.450647147984\n",
      "59\n",
      "train loss:0.549229264237\n",
      "60\n",
      "train loss:0.483459343908\n",
      "61\n",
      "train loss:0.518949420081\n",
      "62\n",
      "train loss:0.422550961116\n",
      "63\n",
      "train loss:0.388949486805\n",
      "64\n",
      "train loss:0.583179850077\n",
      "65\n",
      "train loss:0.511628709044\n",
      "66\n",
      "train loss:0.666232703303\n",
      "67\n",
      "train loss:0.462696309111\n",
      "68\n",
      "train loss:0.544095211768\n",
      "69\n",
      "train loss:0.514000027333\n",
      "70\n",
      "train loss:0.480543045059\n",
      "71\n",
      "train loss:0.524840364634\n",
      "72\n",
      "train loss:0.49196717845\n",
      "73\n",
      "train loss:0.422132731167\n",
      "74\n",
      "train loss:0.545018358981\n",
      "75\n",
      "train loss:0.543850465117\n",
      "76\n",
      "train loss:0.466493755735\n",
      "77\n",
      "train loss:0.451270011544\n",
      "78\n",
      "train loss:0.43230661666\n",
      "79\n",
      "train loss:0.55672619206\n",
      "80\n",
      "train loss:0.463420827917\n",
      "81\n",
      "train loss:0.451361930194\n",
      "82\n",
      "train loss:0.516947882893\n",
      "83\n",
      "train loss:0.52190238054\n",
      "84\n",
      "train loss:0.520652552136\n",
      "85\n",
      "train loss:0.456274841681\n",
      "86\n",
      "train loss:0.525457787316\n",
      "87\n",
      "train loss:0.398578882403\n",
      "88\n",
      "train loss:0.394511431306\n",
      "89\n",
      "train loss:0.528544674596\n",
      "90\n",
      "train loss:0.440360702587\n",
      "91\n",
      "train loss:0.438143653851\n",
      "92\n",
      "train loss:0.440850791332\n",
      "93\n",
      "train loss:0.506822242756\n",
      "94\n",
      "train loss:0.513235858644\n",
      "95\n",
      "train loss:0.464418324766\n",
      "96\n",
      "train loss:0.426438228248\n",
      "97\n",
      "train loss:0.428700008565\n",
      "98\n",
      "train loss:0.387262263071\n",
      "99\n",
      "train loss:0.47113473565\n",
      "100\n",
      "train loss:0.482921252811\n",
      "101\n",
      "train loss:0.488042852056\n",
      "102\n",
      "train loss:0.483593600452\n",
      "103\n",
      "train loss:0.47608137858\n",
      "104\n",
      "train loss:0.440054347577\n",
      "105\n",
      "train loss:0.476262488502\n",
      "106\n",
      "train loss:0.423797571229\n",
      "107\n",
      "train loss:0.497233858729\n",
      "108\n",
      "train loss:0.524748665555\n",
      "109\n",
      "train loss:0.587839874001\n",
      "110\n",
      "train loss:0.440274140613\n",
      "111\n",
      "train loss:0.463500313071\n",
      "112\n",
      "train loss:0.490925787541\n",
      "113\n",
      "train loss:0.491559836265\n",
      "114\n",
      "train loss:0.433663219306\n",
      "115\n",
      "train loss:0.535690741632\n",
      "116\n",
      "train loss:0.538218525802\n",
      "117\n",
      "train loss:0.5905654025\n",
      "118\n",
      "train loss:0.414637647366\n",
      "119\n",
      "train loss:0.409098685129\n",
      "120\n",
      "train loss:0.543984507694\n",
      "121\n",
      "train loss:0.428672346204\n",
      "122\n",
      "train loss:0.488337553797\n",
      "123\n",
      "train loss:0.418691906371\n",
      "124\n",
      "train loss:0.479570249638\n",
      "125\n",
      "train loss:0.480106989896\n",
      "126\n",
      "train loss:0.444704737007\n",
      "127\n",
      "train loss:0.490483221238\n",
      "128\n",
      "train loss:0.409474670002\n",
      "129\n",
      "train loss:0.485073640449\n",
      "130\n",
      "train loss:0.481033329191\n",
      "131\n",
      "train loss:0.502352604484\n",
      "132\n",
      "train loss:0.442657709895\n",
      "133\n",
      "train loss:0.461381817869\n",
      "134\n",
      "train loss:0.540329759859\n",
      "135\n",
      "train loss:0.46027646447\n",
      "136\n",
      "train loss:0.526669724845\n",
      "137\n",
      "train loss:0.398954488807\n",
      "138\n",
      "train loss:0.423475100971\n",
      "139\n",
      "train loss:0.502144739704\n",
      "140\n",
      "train loss:0.378280663577\n",
      "141\n",
      "train loss:0.469245153864\n",
      "142\n",
      "train loss:0.429251097506\n",
      "143\n",
      "train loss:0.49036492457\n",
      "144\n",
      "train loss:0.414778262286\n",
      "145\n",
      "train loss:0.546556042055\n",
      "146\n",
      "train loss:0.469089972017\n",
      "147\n",
      "train loss:0.50527337153\n",
      "148\n",
      "train loss:0.479469659146\n",
      "149\n",
      "train loss:0.565939717864\n",
      "150\n",
      "train loss:0.521841075504\n",
      "151\n",
      "train loss:0.495953601153\n",
      "152\n",
      "train loss:0.495766102578\n",
      "153\n",
      "train loss:0.434534771375\n",
      "154\n",
      "train loss:0.470264156374\n",
      "155\n",
      "train loss:0.557442781715\n",
      "156\n",
      "train loss:0.463080310855\n",
      "157\n",
      "train loss:0.467550344827\n",
      "158\n",
      "train loss:0.432805336637\n",
      "159\n",
      "train loss:0.440207196251\n",
      "160\n",
      "train loss:0.422740736478\n",
      "161\n",
      "train loss:0.495547746112\n",
      "162\n",
      "train loss:0.448540922366\n",
      "163\n",
      "train loss:0.462047293548\n",
      "164\n",
      "train loss:0.383785430225\n",
      "165\n",
      "train loss:0.456287801042\n",
      "166\n",
      "train loss:0.522513484661\n",
      "167\n",
      "train loss:0.529049581677\n",
      "168\n",
      "train loss:0.455091247951\n",
      "169\n",
      "train loss:0.512550488271\n",
      "170\n",
      "train loss:0.393659876498\n",
      "171\n",
      "train loss:0.532018571472\n",
      "172\n",
      "train loss:0.463579958099\n",
      "173\n",
      "train loss:0.444487932632\n",
      "174\n",
      "train loss:0.460502764525\n",
      "175\n",
      "train loss:0.496528661673\n",
      "176\n",
      "train loss:0.537633520015\n",
      "177\n",
      "train loss:0.505694610048\n",
      "178\n",
      "train loss:0.480352475847\n",
      "179\n",
      "train loss:0.418757468225\n",
      "180\n",
      "train loss:0.462820676043\n",
      "181\n",
      "train loss:0.349168850626\n",
      "182\n",
      "train loss:0.415219477128\n",
      "183\n",
      "train loss:0.466842813714\n",
      "184\n",
      "train loss:0.50782975201\n",
      "185\n",
      "train loss:0.444228139876\n",
      "186\n",
      "train loss:0.483142628421\n",
      "187\n",
      "train loss:0.573007943376\n",
      "188\n",
      "train loss:0.520943213811\n",
      "189\n",
      "train loss:0.423454079192\n",
      "190\n",
      "train loss:0.392321707059\n",
      "191\n",
      "train loss:0.417121103712\n",
      "192\n",
      "train loss:0.421914172605\n",
      "193\n",
      "train loss:0.491177405266\n",
      "194\n",
      "train loss:0.49584708921\n",
      "195\n",
      "train loss:0.610734810186\n",
      "196\n",
      "train loss:0.416114076931\n",
      "197\n",
      "train loss:0.526595145654\n",
      "198\n",
      "train loss:0.431619131896\n",
      "199\n",
      "train loss:0.531500197839\n",
      "200\n",
      "train loss:0.557263791042\n",
      "201\n",
      "train loss:0.512119355579\n",
      "202\n",
      "train loss:0.413709952517\n",
      "203\n",
      "train loss:0.475831595314\n",
      "204\n",
      "train loss:0.452864733579\n",
      "205\n",
      "train loss:0.550839712756\n",
      "206\n",
      "train loss:0.453814995821\n",
      "207\n",
      "train loss:0.435257202342\n",
      "208\n",
      "train loss:0.473471645793\n",
      "209\n",
      "train loss:0.591710185894\n",
      "210\n",
      "train loss:0.44297059099\n",
      "211\n",
      "train loss:0.444798662117\n",
      "212\n",
      "train loss:0.467569719099\n",
      "213\n",
      "train loss:0.409611956321\n",
      "214\n",
      "train loss:0.48230960834\n",
      "215\n",
      "train loss:0.63640648413\n",
      "216\n",
      "train loss:0.525311443665\n",
      "217\n",
      "train loss:0.481762838849\n",
      "218\n",
      "train loss:0.477907673186\n",
      "219\n",
      "train loss:0.563700688236\n",
      "220\n",
      "train loss:0.552281015854\n",
      "221\n",
      "train loss:0.51671460723\n",
      "222\n",
      "train loss:0.412501039112\n",
      "223\n",
      "train loss:0.457668795973\n",
      "224\n",
      "train loss:0.517193258195\n",
      "225\n",
      "train loss:0.420913377968\n",
      "226\n",
      "train loss:0.506718250419\n",
      "227\n",
      "train loss:0.491284780623\n",
      "228\n",
      "train loss:0.486986360782\n",
      "229\n",
      "train loss:0.446217515341\n",
      "230\n",
      "train loss:0.462989147526\n",
      "231\n",
      "train loss:0.532283822685\n",
      "232\n",
      "train loss:0.446513792112\n",
      "233\n",
      "train loss:0.400668801127\n",
      "234\n",
      "train loss:0.474051530679\n",
      "235\n",
      "train loss:0.457458340763\n",
      "236\n",
      "train loss:0.436582772354\n",
      "237\n",
      "train loss:0.488571200941\n",
      "238\n",
      "train loss:0.42466150694\n",
      "239\n",
      "train loss:0.454657078526\n",
      "240\n",
      "train loss:0.595473762973\n",
      "241\n",
      "train loss:0.409650280249\n",
      "242\n",
      "train loss:0.430146224204\n",
      "243\n",
      "train loss:0.441283243911\n",
      "244\n",
      "train loss:0.530955878178\n",
      "245\n",
      "train loss:0.393671640503\n",
      "246\n",
      "train loss:0.445442207449\n",
      "247\n",
      "train loss:0.477982134659\n",
      "248\n",
      "train loss:0.407951686603\n",
      "249\n",
      "train loss:0.532808803101\n",
      "250\n",
      "train loss:0.415837848592\n",
      "251\n",
      "train loss:0.554075936084\n",
      "252\n",
      "train loss:0.435615790126\n",
      "253\n",
      "train loss:0.471820402422\n",
      "254\n",
      "train loss:0.451026990391\n",
      "255\n",
      "train loss:0.502691660235\n",
      "256\n",
      "train loss:0.448661219309\n",
      "257\n",
      "train loss:0.459207190149\n",
      "258\n",
      "train loss:0.464514776932\n",
      "259\n",
      "train loss:0.496602348715\n",
      "260\n",
      "train loss:0.455275267482\n",
      "261\n",
      "train loss:0.379698106209\n",
      "262\n",
      "train loss:0.406240218349\n",
      "263\n",
      "train loss:0.436551707922\n",
      "264\n",
      "train loss:0.585030576738\n",
      "265\n",
      "train loss:0.43762605745\n",
      "266\n",
      "train loss:0.449512421298\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.992383512545\n"
     ]
    }
   ],
   "source": [
    "# # coding: utf-8\n",
    "# # 대문자와 숫자만.(대문자 알파뱃 정리함.)\n",
    "# import sys, os\n",
    "# sys.path.append(os.pardir)  #| 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from dataset.ocrdata_Upper_Digit import load_dataset\n",
    "# from dataset.mnist import load_mnist\n",
    "# from net.deep_convnet2 import DeepConvNet\n",
    "# from common.trainer import Trainer\n",
    "# import pickle\n",
    "# (x_train, t_train), (x_test, t_test) = load_dataset()\n",
    "# save_path = '/Users/yunsu/Desktop/book_reading/study_by_self/load/ocrparams/revised_data/E_UPPER_D_FNx1.pk1'\n",
    "# file_path = None\n",
    "# network = DeepConvNet(output_size=36, file_path=file_path)  \n",
    "# trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "#                   epochs=10, mini_batch_size=100,\n",
    "#                   optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "#                   evaluate_sample_num_per_epoch=1000)\n",
    "# trainer.train(save_path=save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loadind data pickles\n",
      "done\n",
      "[   9  288  288  576  576 1152 2048  100]\n",
      "0\n",
      "train loss:3.35738562244\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-5ba71e0bdaaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                   \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                   evaluate_sample_num_per_epoch=1000)\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yunsu/Desktop/book_reading/study_by_self/common/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, save_path)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yunsu/Desktop/book_reading/study_by_self/common/trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, save_path)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mx_test_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_test_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_train_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_test_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yunsu/Desktop/book_reading/study_by_self/net/deep_convnet2.py\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(self, x, t, batch_size)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yunsu/Desktop/book_reading/study_by_self/net/deep_convnet2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, train_flg)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_flg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yunsu/Desktop/book_reading/study_by_self/common/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mout_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mFW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim2col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mcol_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "# 숫자 + 대문자 (C, S, Z 제외) + 소문자\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  #| 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.ocrdata3 import load_dataset\n",
    "from dataset.mnist import load_mnist\n",
    "from net.deep_convnet2 import DeepConvNet\n",
    "from common.trainer import Trainer\n",
    "import pickle\n",
    "(x_train, t_train), (x_test, t_test) = load_dataset()\n",
    "save_path = '/Users/yunsu/Desktop/book_reading/study_by_self/load/ocrparams/revised_data/E_D_Excep_FNx1.pk1'\n",
    "file_path = None\n",
    "network = DeepConvNet(output_size=59, file_path=file_path)  \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=10, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train(save_path=save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path = '/Users/yunsu/Desktop/book_reading/study_by_self/load/ocrparams/revised_data/E_D_Excep_FNx1.pk1'\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading param pickles\n",
      "done\n",
      "loadind data pickles\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  #| 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.ocrdata3 import load_dataset\n",
    "from dataset.mnist import load_mnist\n",
    "from net.deep_convnet2 import DeepConvNet\n",
    "from common.trainer import Trainer\n",
    "import pickle\n",
    "\n",
    "network = DeepConvNet(file_path=save_path, output_size=59)\n",
    "(x_train, t_train), (x_test, t_test) = load_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11011,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predic = np.argmax(network.predict(x_test[:2000]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = np.equal(predic, t_test[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False], dtype=bool)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "True ^mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre = predic[True ^ mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/var/pyenv/versions/3.4.3/envs/selenium/lib/python3.4/site-packages/ipykernel/__main__.py:1: VisibleDeprecationWarning: boolean index did not match indexed array along dimension 0; dimension is 11011 but corresponding boolean dimension is 2000\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "errors = t_test[True ^ mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 3,\n",
       " 4: 1,\n",
       " 13: 1,\n",
       " 16: 1,\n",
       " 17: 16,\n",
       " 20: 3,\n",
       " 21: 2,\n",
       " 22: 2,\n",
       " 24: 1,\n",
       " 25: 3,\n",
       " 26: 3,\n",
       " 28: 1,\n",
       " 29: 6,\n",
       " 30: 6,\n",
       " 31: 6,\n",
       " 32: 3,\n",
       " 33: 4,\n",
       " 37: 1,\n",
       " 39: 3,\n",
       " 40: 2,\n",
       " 44: 5,\n",
       " 45: 1,\n",
       " 46: 2,\n",
       " 48: 3,\n",
       " 51: 12,\n",
       " 53: 2,\n",
       " 54: 12,\n",
       " 55: 11,\n",
       " 56: 6,\n",
       " 58: 7}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_dic = {}\n",
    "for key in pre:\n",
    "    try:\n",
    "        pre_dic[key] += 1\n",
    "    except:\n",
    "        pre_dic[key] = 1\n",
    "pre_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2,\n",
       " 1: 1,\n",
       " 9: 2,\n",
       " 13: 1,\n",
       " 17: 4,\n",
       " 18: 2,\n",
       " 21: 2,\n",
       " 24: 2,\n",
       " 27: 12,\n",
       " 28: 2,\n",
       " 29: 11,\n",
       " 30: 11,\n",
       " 31: 6,\n",
       " 33: 6,\n",
       " 37: 1,\n",
       " 38: 2,\n",
       " 43: 1,\n",
       " 44: 18,\n",
       " 45: 2,\n",
       " 46: 4,\n",
       " 47: 2,\n",
       " 48: 1,\n",
       " 50: 4,\n",
       " 51: 2,\n",
       " 53: 1,\n",
       " 54: 7,\n",
       " 55: 6,\n",
       " 56: 7,\n",
       " 57: 3,\n",
       " 58: 4}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_dic = {}\n",
    "for key in errors:\n",
    "    try:\n",
    "        error_dic[key] += 1\n",
    "    except:\n",
    "        error_dic[key] = 1\n",
    "error_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic[54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "except_upper = 'ABDEFGHIJKLMNOPQRTUVWXY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic = list(range(10)) + list(except_upper) + list(string.ascii_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "sys.path.append(\"/Users/yunsu/mysite-projects/selenium\")\n",
    "from PIL import Image\n",
    "from python_ocr import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"/Users/yunsu/Downloads/English/Fnt2/Sample058/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-15aab47318a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_cordinate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yunsu/mysite-projects/selenium/python_ocr/crop/cordinate.py\u001b[0m in \u001b[0;36my_cordinate\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 문장들을 줄 단위로 자르기 위해서 만든 함수. 각 줄의 시작 좌표리스트(linetop)과 각 줄의 끝 좌표리스트(linebot)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0my_cordinate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_into_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mlinetop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yunsu/mysite-projects/selenium/python_ocr/conversion/conversions.py\u001b[0m in \u001b[0;36mimage_into_array\u001b[0;34m(img_url, size, threshold)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.2989\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.5870\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.1140\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdet\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/var/pyenv/versions/3.4.3/envs/selenium/lib/python3.4/site-packages/numpy/core/_methods.py:82: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  warnings.warn(\"Degrees of freedom <= 0 for slice\", RuntimeWarning)\n",
      "/usr/local/var/pyenv/versions/3.4.3/envs/selenium/lib/python3.4/site-packages/numpy/core/_methods.py:94: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/usr/local/var/pyenv/versions/3.4.3/envs/selenium/lib/python3.4/site-packages/numpy/core/_methods.py:116: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "std = np.std(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "std = 5.1078742850631542"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.107874285063154"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = 16.296259842519685"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-72c5975579ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_cordinate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmean\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yunsu/mysite-projects/selenium/python_ocr/crop/cordinate.py\u001b[0m in \u001b[0;36my_cordinate\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 문장들을 줄 단위로 자르기 위해서 만든 함수. 각 줄의 시작 좌표리스트(linetop)과 각 줄의 끝 좌표리스트(linebot)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0my_cordinate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_into_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mlinetop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yunsu/mysite-projects/selenium/python_ocr/conversion/conversions.py\u001b[0m in \u001b[0;36mimage_into_array\u001b[0;34m(img_url, size, threshold)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mzero_or_one_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mzero_or_one_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for file in os.listdir(path):\n",
    "    pos = y_cordinate(path + file)\n",
    "    if pos[0][0] > mean  + 2 * std:\n",
    "        img = Image.open(path + file)\n",
    "        img.save(path + str(i) + \".png\" )\n",
    "        print(i)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.1078742850631542"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.1078742850631542"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Z'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic[35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 28, 35)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12, 28 ,35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = []\n",
    "a.append(1)\n",
    "a.append(2)\n",
    "a.append(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.remove(a[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = t_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic = list(range(10)) + list(string.ascii_uppercase) + list(string.ascii_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'k'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic[y] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABMUlEQVR4nLWSvyvEYRzHX99v3xzf\nqztdSpJuELMfqRtksVmc/8Bwiy4ji0EsymgxYMKCVUoskrjBoiSRLEqd6wad01fehuc5XTxu81ne\n7+fzej6fPp+nxxN/h9+ANYYBEJ1V7Kl9oFQQgJ9JAkg6/r6aLC9aNy9JPpAets1bJsJMDwCtvdhK\nRf0GTkl66YNg5kGSFAAECQPbgFSIv5qrn9Yu+wmsnTObq80gSRoxfk66SjFalY0fe1bzJeKxPx5h\n6QSObp0wVlgGKttOWJyuAuy9ueDdNQA3py6YCM1Gmy7YPWj08MkBw3Gjz/sOGI2ZvmzpN/zoGjLm\n4rIOejWcNfK+WwcjW0k2btxOGQBJxbxNda4sNNdaPEpC0gGOWJfkCcobrzaTDu6ta5rsAO9/PvUX\nO8NxYF+Oue8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x11C60B278>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/Users/yunsu/Desktop/book_reading/study_by_self/load/ocrparams2.pk1','rb') as f:\n",
    "    net = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_test.shape[0] + x_train.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network.layers[4].dbeta.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/Users/yunsu/Desktop/book_reading/study_by_self/load/ocrparams.pk1', 'wb') as f:\n",
    "    pickle.dump(net, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = x_train[0:100]\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "activation = []\n",
    "for idx, layer in enumerate(network.layers):\n",
    "    x = layer.forward(x)\n",
    "    activation.append(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, a in activations.items():\n",
    "    plt.subplot(1, len(activations), i+1)\n",
    "    plt.title(str(i+1) + \"-layer\")\n",
    "    if i != 0: plt.yticks([], [])\n",
    "    # plt.xlim(0.1, 1)\n",
    "    # plt.ylim(0, 7000)\n",
    "    plt.hist(a.flatten(), 30, range=(0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiEAAAFyCAYAAAAj5QpDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ff1a898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "    \n",
    "input_data = np.random.randn(1000, 100)  # 1000개의 데이터\n",
    "node_num = 100  # 각 은닉층의 노드(뉴런) 수\n",
    "hidden_layer_size = 5  # 은닉층이 5개\n",
    "activations = {}  # 이곳에 활성화 결과를 저장\n",
    "\n",
    "x = input_data\n",
    "\n",
    "for i in range(hidden_layer_size):\n",
    "    if i != 0:\n",
    "        x = activations[i-1]\n",
    "\n",
    "    # 초깃값을 다양하게 바꿔가며 실험해보자！\n",
    "    w = np.random.randn(node_num, node_num) * 1\n",
    "    w = np.random.randn(node_num, node_num) * 0.01\n",
    "    w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
    "    w = np.random.randn(node_num, node_num) * np.sqrt(2 / node_num)\n",
    "\n",
    "\n",
    "    a = np.dot(x, w)\n",
    "\n",
    "\n",
    "    # 활성화 함수도 바꿔가며 실험해보자！\n",
    "#     z = sigmoid(a)\n",
    "    z = ReLU(a)\n",
    "    # z = tanh(a)\n",
    "\n",
    "    activations[i] = z\n",
    "\n",
    "# 히스토그램 그리기\n",
    "for i, a in activations.items():\n",
    "    plt.subplot(1, len(activations), i+1)\n",
    "    plt.title(str(i+1) + \"-layer\")\n",
    "    if i != 0: plt.yticks([], [])\n",
    "    # plt.xlim(0.1, 1)\n",
    "    # plt.ylim(0, 7000)\n",
    "    plt.hist(a.flatten(), 30, range=(0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loadind data pickles\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  #| 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.ocrdata2 import load_dataset\n",
    "from dataset.mnist import load_mnist\n",
    "from net.deep_convnet2 import DeepConvNet\n",
    "from common.trainer import Trainer\n",
    "import pickle\n",
    "(x_train, t_train), (x_test, t_test) = load_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50361, 1, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
